{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Challenge - Πρόβλεψη Πληρότητας Πτήσεων\n",
    "\n",
    "Στα πλαίσια της εργασίας του μαθήματος \"Εξόρυξη Γνώσης από Βάσεις Δεδομένων και τον Παγκόσμιο Ιστό\", θα δουλέψετε πάνω σε ένα πρόβλημα κατηγοριοποίησης. Συγκεκριμένα, σας δίνεται ένα σύνολο δεδομένων το οποίο αποτελείται από μερικές χιλιάδες πτήσεις, όπου κάθε πτήση περιγράφεται απο ένα σύνολο μεταβλητών (αεροδρόμιο αναχώρησης, αεροδρόμιο άφιξης, κτλ). Κάθε πτήση χαρακτηρίζεται επίσης από μια μεταβλητή που σχετίζεται με τον αριθμό των επιβατών της πτήσης (π.χ. κάθε τιμή της μεταβλητής σχετίζεται με ενα εύρος πλήθους επιβατών). Για κάποιες πτήσεις, η τιμή της μεταβλητής  είναι γνωστή, ενώ για άλλες όχι. Στόχος σας είναι να προβλέψετε την τιμή της μεταβλητής για τις πτήσεις για τις οποίες δεν είναι διαθέσιμη.\n",
    "\n",
    "### Σύνολο Δεδομένων\n",
    "\n",
    "Το αρχείο με όνομα `train.csv` περιέχει τα δεδομένα εκπαίδευσης (training set) του προβλήματος, ενώ το αρχείο `test.csv` περιέχει τα δεδομένα ελέγχου (test set) του προβλήματος. Κάθε γραμμή των δυο αυτών αρχείων αντιστοιχεί σε μια πτήση η οποία χαρακτηρίζεται από τις εξής μεταβλητές:\n",
    "\n",
    "Μεταβλητή | Περιγραφή\n",
    "--- | --- \n",
    "DateOfDeparture | Ημερομηνία αναχώρησης\n",
    "Departure | Κωδικός αεροδρομίου αναχώρησης\n",
    "CityDeparture | Πόλη αναχώρησης\n",
    "LongitudeDeparture \t | Γεωγραφικό μήκος αεροδρομίου αναχώρησης\n",
    "LatitudeDeparture \t | Γεωγραφικό πλάτος αεροδρομίου αναχώρησης\n",
    "Arrival | Κωδικός αεροδρομίου άφιξης\n",
    "CityArrival | Πόλη άφιξης\n",
    "LongitudeArrival | Γεωγραφικό μήκος αεροδρομίου άφιξης\n",
    "LatitudeArrival | Γεωγραφικό πλάτος αεροδρομίου άφιξης\n",
    "WeeksToDeparture | Πόσες εβδομάδες πριν την αναχώρηση της πτήσης κατά μέσο όρο έκλεισαν οι επιβάτες τα εισητήριά τους\n",
    "std_wtd | Τυπική απόκλιση για το παραπάνω \n",
    "Το training set περιέχει μια επιπλέον μεταβλητή (`PAX`) η οποία έχει σχέση με τον αριθμό των επιβατών της πτήσης. Η μεταβλητή αυτή παίρνει 8 διαφορετικές τιμές (τιμές από 0 έως 7 οπότε 8 κατηγορίες συνολικά). Κάθε κατηγορία υποδηλώνει πόσοι περίπου επιβάτες χρησιμοποίησαν την πτήση. Οι αριθμοί στις κατηγορίες έχουν ανατεθεί με τυχαίο τρόπο, οπότε μην θεωρήσετε ότι η κατηγορία 0 υποδηλώνει πολύ λίγους επιβάτες ενώ η κατηγορία 7 πάρα πολλούς επιβάτες. Η μεταβλητή `PAX` λείπει από το test set καθώς πρόκειται για την μεταβλητή που πρέπει να προβλέψετε στα πλαίσια της παρούσας εργασίας.\n",
    "\n",
    "Παρακάτω σας δίνεται κώδικας ο οποίος φορτώνει τα δεδομένα εκπαίδευσης σε ένα DataFrame της βιβλιοθήκης Pandas και τυπώνει τις πρώτες 5 γραμμές. Οπότε μπορείτε να δείτε τις 12 μεταβλητές του προβλήματος."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DateOfDeparture</th>\n",
       "      <th>Departure</th>\n",
       "      <th>CityDeparture</th>\n",
       "      <th>LongitudeDeparture</th>\n",
       "      <th>LatitudeDeparture</th>\n",
       "      <th>Arrival</th>\n",
       "      <th>CityArrival</th>\n",
       "      <th>LongitudeArrival</th>\n",
       "      <th>LatitudeArrival</th>\n",
       "      <th>WeeksToDeparture</th>\n",
       "      <th>std_wtd</th>\n",
       "      <th>PAX</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2011-12-05</td>\n",
       "      <td>ORD</td>\n",
       "      <td>Chicago</td>\n",
       "      <td>41.978603</td>\n",
       "      <td>-87.904842</td>\n",
       "      <td>EWR</td>\n",
       "      <td>Newark</td>\n",
       "      <td>40.692500</td>\n",
       "      <td>-74.168667</td>\n",
       "      <td>8.352941</td>\n",
       "      <td>5.667243</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2013-03-01</td>\n",
       "      <td>ATL</td>\n",
       "      <td>Atlanta</td>\n",
       "      <td>33.636719</td>\n",
       "      <td>-84.428067</td>\n",
       "      <td>LGA</td>\n",
       "      <td>New York</td>\n",
       "      <td>40.777245</td>\n",
       "      <td>-73.872608</td>\n",
       "      <td>10.421053</td>\n",
       "      <td>10.001754</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2012-02-06</td>\n",
       "      <td>ORD</td>\n",
       "      <td>Chicago</td>\n",
       "      <td>41.978603</td>\n",
       "      <td>-87.904842</td>\n",
       "      <td>BOS</td>\n",
       "      <td>Boston</td>\n",
       "      <td>42.364347</td>\n",
       "      <td>-71.005181</td>\n",
       "      <td>9.250000</td>\n",
       "      <td>7.136821</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2012-12-10</td>\n",
       "      <td>ORD</td>\n",
       "      <td>Chicago</td>\n",
       "      <td>41.978603</td>\n",
       "      <td>-87.904842</td>\n",
       "      <td>SFO</td>\n",
       "      <td>San Francisco</td>\n",
       "      <td>37.618972</td>\n",
       "      <td>-122.374889</td>\n",
       "      <td>8.666667</td>\n",
       "      <td>7.404291</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2012-04-13</td>\n",
       "      <td>SFO</td>\n",
       "      <td>San Francisco</td>\n",
       "      <td>37.618972</td>\n",
       "      <td>-122.374889</td>\n",
       "      <td>JFK</td>\n",
       "      <td>New York</td>\n",
       "      <td>40.639751</td>\n",
       "      <td>-73.778925</td>\n",
       "      <td>14.037037</td>\n",
       "      <td>9.858544</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  DateOfDeparture Departure  CityDeparture  LongitudeDeparture  \\\n",
       "0      2011-12-05       ORD        Chicago           41.978603   \n",
       "1      2013-03-01       ATL        Atlanta           33.636719   \n",
       "2      2012-02-06       ORD        Chicago           41.978603   \n",
       "3      2012-12-10       ORD        Chicago           41.978603   \n",
       "4      2012-04-13       SFO  San Francisco           37.618972   \n",
       "\n",
       "   LatitudeDeparture Arrival    CityArrival  LongitudeArrival  \\\n",
       "0         -87.904842     EWR         Newark         40.692500   \n",
       "1         -84.428067     LGA       New York         40.777245   \n",
       "2         -87.904842     BOS         Boston         42.364347   \n",
       "3         -87.904842     SFO  San Francisco         37.618972   \n",
       "4        -122.374889     JFK       New York         40.639751   \n",
       "\n",
       "   LatitudeArrival  WeeksToDeparture    std_wtd  PAX  \n",
       "0       -74.168667          8.352941   5.667243    7  \n",
       "1       -73.872608         10.421053  10.001754    7  \n",
       "2       -71.005181          9.250000   7.136821    7  \n",
       "3      -122.374889          8.666667   7.404291    7  \n",
       "4       -73.778925         14.037037   9.858544    7  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_train = pd.read_csv('dataset/train.csv')\n",
    "df_train.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Επίσης, όσον αφορά τις διαστάσεις του training set, όπως βλέπετε παρακάτω, αποτελείται από 8899 γραμμές και 12 στήλες."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8899, 12)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dateutil.parser as p\n",
    "import numpy as np\n",
    "\n",
    "dates = df_train[['DateOfDeparture']]\n",
    "\n",
    "datetime=[p.parse(dates.iloc[i]['DateOfDeparture']) for i in range(dates.shape[0])]\n",
    "\n",
    "date_number=[datetime[i].isoweekday() for i in range(dates.shape[0])]\n",
    "\n",
    "date_month=[datetime[i].isocalendar()[1] for i in range(dates.shape[0])]\n",
    "\n",
    "date_number=pd.DataFrame(date_number,columns=['Date'])\n",
    "date_month=pd.DataFrame(date_month,columns=['Week'])\n",
    "\n",
    "df_train['Date']=date_number.values\n",
    "df_train['Week']=date_month.values\n",
    "\n",
    "\n",
    "score = np.zeros((dates.shape[0]),dtype=int)\n",
    "for i in range(dates.shape[0]):\n",
    "    if datetime[i].isoweekday()==6 or datetime[i].isoweekday==7:\n",
    "        score[i]=1\n",
    "    \n",
    "scor=pd.DataFrame(score,columns=['Weekend'])\n",
    "df_train['Weekend']=scor.values\n",
    "\n",
    "\n",
    "score = np.empty((dates.shape[0]),dtype=str)\n",
    "for i in range(dates.shape[0]):\n",
    "    if datetime[i].isoweekday() == 4 and 22 <= datetime[i].day < 29 and datetime[i].month==11:\n",
    "        score[i]='Independance Day'\n",
    "    if 24 <= datetime[i].day <= 25 and datetime[i].month==12:\n",
    "        score[i]='Christmas'\n",
    "    if (datetime[i].day == 31 and datetime[i].month==12) or (datetime[i].day == 1 and datetime[i].month==1):\n",
    "        score[i]='New Year'\n",
    "    \n",
    "scor=pd.DataFrame(score,columns=['Celebs'])\n",
    "df_train['Celebs']=scor.values\n",
    "\n",
    "df_train[['WeeksToDeparture']]= pd.cut(df_train['WeeksToDeparture'],  [0,3.6,4.01,6,8,9,10,11,12,13,14,15,16,17,18.3,20.5,24],labels=[ '0-3', '3-4','4-6','6-8','8-9','9-10','10-11','11-12','12-13','13-14','14-15','15-16','16-17','17-18','18-20','20-24'])\n",
    "\n",
    "cols = df_train.columns.tolist()\n",
    "cols =cols[-4:]+cols[:-4]\n",
    "df_train=df_train[cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Week</th>\n",
       "      <th>Weekend</th>\n",
       "      <th>Celebs</th>\n",
       "      <th>DateOfDeparture</th>\n",
       "      <th>Departure</th>\n",
       "      <th>CityDeparture</th>\n",
       "      <th>LongitudeDeparture</th>\n",
       "      <th>LatitudeDeparture</th>\n",
       "      <th>Arrival</th>\n",
       "      <th>CityArrival</th>\n",
       "      <th>LongitudeArrival</th>\n",
       "      <th>LatitudeArrival</th>\n",
       "      <th>WeeksToDeparture</th>\n",
       "      <th>std_wtd</th>\n",
       "      <th>PAX</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>49</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td>2011-12-05</td>\n",
       "      <td>ORD</td>\n",
       "      <td>Chicago</td>\n",
       "      <td>41.978603</td>\n",
       "      <td>0.671019</td>\n",
       "      <td>EWR</td>\n",
       "      <td>Newark</td>\n",
       "      <td>40.692500</td>\n",
       "      <td>0.938417</td>\n",
       "      <td>8-9</td>\n",
       "      <td>5.667243</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td>2013-03-01</td>\n",
       "      <td>ATL</td>\n",
       "      <td>Atlanta</td>\n",
       "      <td>33.636719</td>\n",
       "      <td>0.738700</td>\n",
       "      <td>LGA</td>\n",
       "      <td>New York</td>\n",
       "      <td>40.777245</td>\n",
       "      <td>0.944181</td>\n",
       "      <td>10-11</td>\n",
       "      <td>10.001754</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td>2012-02-06</td>\n",
       "      <td>ORD</td>\n",
       "      <td>Chicago</td>\n",
       "      <td>41.978603</td>\n",
       "      <td>0.671019</td>\n",
       "      <td>BOS</td>\n",
       "      <td>Boston</td>\n",
       "      <td>42.364347</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>9-10</td>\n",
       "      <td>7.136821</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>50</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td>2012-12-10</td>\n",
       "      <td>ORD</td>\n",
       "      <td>Chicago</td>\n",
       "      <td>41.978603</td>\n",
       "      <td>0.671019</td>\n",
       "      <td>SFO</td>\n",
       "      <td>San Francisco</td>\n",
       "      <td>37.618972</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>8-9</td>\n",
       "      <td>7.404291</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td>2012-04-13</td>\n",
       "      <td>SFO</td>\n",
       "      <td>San Francisco</td>\n",
       "      <td>37.618972</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>JFK</td>\n",
       "      <td>New York</td>\n",
       "      <td>40.639751</td>\n",
       "      <td>0.946004</td>\n",
       "      <td>14-15</td>\n",
       "      <td>9.858544</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Date  Week  Weekend Celebs DateOfDeparture Departure  CityDeparture  \\\n",
       "0     1    49        0             2011-12-05       ORD        Chicago   \n",
       "1     5     9        0             2013-03-01       ATL        Atlanta   \n",
       "2     1     6        0             2012-02-06       ORD        Chicago   \n",
       "3     1    50        0             2012-12-10       ORD        Chicago   \n",
       "4     5    15        0             2012-04-13       SFO  San Francisco   \n",
       "\n",
       "   LongitudeDeparture  LatitudeDeparture Arrival    CityArrival  \\\n",
       "0           41.978603           0.671019     EWR         Newark   \n",
       "1           33.636719           0.738700     LGA       New York   \n",
       "2           41.978603           0.671019     BOS         Boston   \n",
       "3           41.978603           0.671019     SFO  San Francisco   \n",
       "4           37.618972           0.000000     JFK       New York   \n",
       "\n",
       "   LongitudeArrival  LatitudeArrival WeeksToDeparture    std_wtd  PAX  \n",
       "0         40.692500         0.938417              8-9   5.667243    7  \n",
       "1         40.777245         0.944181            10-11  10.001754    7  \n",
       "2         42.364347         1.000000             9-10   7.136821    7  \n",
       "3         37.618972         0.000000              8-9   7.404291    7  \n",
       "4         40.639751         0.946004            14-15   9.858544    7  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "# Create a minimum and maximum processor object\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "\n",
    "# Create an object to transform the data to fit minmax processor\n",
    "\n",
    "df_train[['LatitudeDeparture']]= min_max_scaler.fit_transform(df_train[['LatitudeDeparture']].values)\n",
    "\n",
    "df_train[['LatitudeArrival']]= min_max_scaler.fit_transform(df_train[['LatitudeArrival']].values)\n",
    "\n",
    "\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Δηλαδή έχετε διαθέσιμα τα δεδομένα απο 8899 πτήσεις για τις οποίες ξέρετε την κατηγορία του αριθμού επιβάτών `PAX`.\n",
    "\n",
    "Αντίθετα, το αρχείο με όνομα `test.csv` περιέχει το test set του προβλήματος. Όπως και στην περίπτωση του training set, σας δίνεται και τώρα κώδικας ο οποίος φορτώνει το test set σε ένα DataFrame της βιβλιοθήκης Pandas και τυπώνει τις πρώτες 5 γραμμές του."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DateOfDeparture</th>\n",
       "      <th>Departure</th>\n",
       "      <th>CityDeparture</th>\n",
       "      <th>LongitudeDeparture</th>\n",
       "      <th>LatitudeDeparture</th>\n",
       "      <th>Arrival</th>\n",
       "      <th>CityArrival</th>\n",
       "      <th>LongitudeArrival</th>\n",
       "      <th>LatitudeArrival</th>\n",
       "      <th>WeeksToDeparture</th>\n",
       "      <th>std_wtd</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2012-10-21</td>\n",
       "      <td>DFW</td>\n",
       "      <td>Dallas-Fort Worth</td>\n",
       "      <td>32.896828</td>\n",
       "      <td>-97.037997</td>\n",
       "      <td>SFO</td>\n",
       "      <td>San Francisco</td>\n",
       "      <td>37.618972</td>\n",
       "      <td>-122.374889</td>\n",
       "      <td>14.600000</td>\n",
       "      <td>11.575837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2012-09-13</td>\n",
       "      <td>LAX</td>\n",
       "      <td>Los Angeles</td>\n",
       "      <td>33.942536</td>\n",
       "      <td>-118.408075</td>\n",
       "      <td>ATL</td>\n",
       "      <td>Atlanta</td>\n",
       "      <td>33.636719</td>\n",
       "      <td>-84.428067</td>\n",
       "      <td>14.730769</td>\n",
       "      <td>13.364304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2012-09-04</td>\n",
       "      <td>ORD</td>\n",
       "      <td>Chicago</td>\n",
       "      <td>41.978603</td>\n",
       "      <td>-87.904842</td>\n",
       "      <td>IAH</td>\n",
       "      <td>Houston</td>\n",
       "      <td>29.984433</td>\n",
       "      <td>-95.341442</td>\n",
       "      <td>8.470588</td>\n",
       "      <td>5.885551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2012-08-13</td>\n",
       "      <td>DEN</td>\n",
       "      <td>Denver</td>\n",
       "      <td>39.861656</td>\n",
       "      <td>-104.673178</td>\n",
       "      <td>PHX</td>\n",
       "      <td>Phoenix</td>\n",
       "      <td>33.434278</td>\n",
       "      <td>-112.011583</td>\n",
       "      <td>8.200000</td>\n",
       "      <td>6.292853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2012-09-10</td>\n",
       "      <td>ORD</td>\n",
       "      <td>Chicago</td>\n",
       "      <td>41.978603</td>\n",
       "      <td>-87.904842</td>\n",
       "      <td>SEA</td>\n",
       "      <td>Seattle</td>\n",
       "      <td>47.449000</td>\n",
       "      <td>-122.309306</td>\n",
       "      <td>12.090909</td>\n",
       "      <td>9.138662</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  DateOfDeparture Departure      CityDeparture  LongitudeDeparture  \\\n",
       "0      2012-10-21       DFW  Dallas-Fort Worth           32.896828   \n",
       "1      2012-09-13       LAX        Los Angeles           33.942536   \n",
       "2      2012-09-04       ORD            Chicago           41.978603   \n",
       "3      2012-08-13       DEN             Denver           39.861656   \n",
       "4      2012-09-10       ORD            Chicago           41.978603   \n",
       "\n",
       "   LatitudeDeparture Arrival    CityArrival  LongitudeArrival  \\\n",
       "0         -97.037997     SFO  San Francisco         37.618972   \n",
       "1        -118.408075     ATL        Atlanta         33.636719   \n",
       "2         -87.904842     IAH        Houston         29.984433   \n",
       "3        -104.673178     PHX        Phoenix         33.434278   \n",
       "4         -87.904842     SEA        Seattle         47.449000   \n",
       "\n",
       "   LatitudeArrival  WeeksToDeparture    std_wtd  \n",
       "0      -122.374889         14.600000  11.575837  \n",
       "1       -84.428067         14.730769  13.364304  \n",
       "2       -95.341442          8.470588   5.885551  \n",
       "3      -112.011583          8.200000   6.292853  \n",
       "4      -122.309306         12.090909   9.138662  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test = pd.read_csv('dataset/test.csv')\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2229\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Week</th>\n",
       "      <th>Weekend</th>\n",
       "      <th>Celebs</th>\n",
       "      <th>DateOfDeparture</th>\n",
       "      <th>Departure</th>\n",
       "      <th>CityDeparture</th>\n",
       "      <th>LongitudeDeparture</th>\n",
       "      <th>LatitudeDeparture</th>\n",
       "      <th>Arrival</th>\n",
       "      <th>CityArrival</th>\n",
       "      <th>LongitudeArrival</th>\n",
       "      <th>LatitudeArrival</th>\n",
       "      <th>WeeksToDeparture</th>\n",
       "      <th>std_wtd</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7</td>\n",
       "      <td>42</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td>2012-10-21</td>\n",
       "      <td>DFW</td>\n",
       "      <td>Dallas-Fort Worth</td>\n",
       "      <td>32.896828</td>\n",
       "      <td>0.493226</td>\n",
       "      <td>SFO</td>\n",
       "      <td>San Francisco</td>\n",
       "      <td>37.618972</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>14-15</td>\n",
       "      <td>11.575837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>37</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td>2012-09-13</td>\n",
       "      <td>LAX</td>\n",
       "      <td>Los Angeles</td>\n",
       "      <td>33.942536</td>\n",
       "      <td>0.077221</td>\n",
       "      <td>ATL</td>\n",
       "      <td>Atlanta</td>\n",
       "      <td>33.636719</td>\n",
       "      <td>0.738700</td>\n",
       "      <td>14-15</td>\n",
       "      <td>13.364304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>36</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td>2012-09-04</td>\n",
       "      <td>ORD</td>\n",
       "      <td>Chicago</td>\n",
       "      <td>41.978603</td>\n",
       "      <td>0.671019</td>\n",
       "      <td>IAH</td>\n",
       "      <td>Houston</td>\n",
       "      <td>29.984433</td>\n",
       "      <td>0.526253</td>\n",
       "      <td>8-9</td>\n",
       "      <td>5.885551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>33</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td>2012-08-13</td>\n",
       "      <td>DEN</td>\n",
       "      <td>Denver</td>\n",
       "      <td>39.861656</td>\n",
       "      <td>0.344594</td>\n",
       "      <td>PHX</td>\n",
       "      <td>Phoenix</td>\n",
       "      <td>33.434278</td>\n",
       "      <td>0.201740</td>\n",
       "      <td>8-9</td>\n",
       "      <td>6.292853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>37</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td>2012-09-10</td>\n",
       "      <td>ORD</td>\n",
       "      <td>Chicago</td>\n",
       "      <td>41.978603</td>\n",
       "      <td>0.671019</td>\n",
       "      <td>SEA</td>\n",
       "      <td>Seattle</td>\n",
       "      <td>47.449000</td>\n",
       "      <td>0.001277</td>\n",
       "      <td>12-13</td>\n",
       "      <td>9.138662</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Date  Week  Weekend Celebs DateOfDeparture Departure      CityDeparture  \\\n",
       "0     7    42        0             2012-10-21       DFW  Dallas-Fort Worth   \n",
       "1     4    37        0             2012-09-13       LAX        Los Angeles   \n",
       "2     2    36        0             2012-09-04       ORD            Chicago   \n",
       "3     1    33        0             2012-08-13       DEN             Denver   \n",
       "4     1    37        0             2012-09-10       ORD            Chicago   \n",
       "\n",
       "   LongitudeDeparture  LatitudeDeparture Arrival    CityArrival  \\\n",
       "0           32.896828           0.493226     SFO  San Francisco   \n",
       "1           33.942536           0.077221     ATL        Atlanta   \n",
       "2           41.978603           0.671019     IAH        Houston   \n",
       "3           39.861656           0.344594     PHX        Phoenix   \n",
       "4           41.978603           0.671019     SEA        Seattle   \n",
       "\n",
       "   LongitudeArrival  LatitudeArrival WeeksToDeparture    std_wtd  \n",
       "0         37.618972         0.000000            14-15  11.575837  \n",
       "1         33.636719         0.738700            14-15  13.364304  \n",
       "2         29.984433         0.526253              8-9   5.885551  \n",
       "3         33.434278         0.201740              8-9   6.292853  \n",
       "4         47.449000         0.001277            12-13   9.138662  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dates = df_test[['DateOfDeparture']]\n",
    "print(dates.shape[0])\n",
    "\n",
    "datetime=[p.parse(dates.iloc[i]['DateOfDeparture']) for i in range(dates.shape[0])]\n",
    "\n",
    "date_number=[datetime[i].isoweekday() for i in range(dates.shape[0])]\n",
    "\n",
    "date_month=[datetime[i].isocalendar()[1] for i in range(dates.shape[0])]\n",
    "\n",
    "\n",
    "date_number=pd.DataFrame(date_number,columns=['Date'])\n",
    "date_month=pd.DataFrame(date_month,columns=['Week'])\n",
    "\n",
    "df_test['Date']=date_number.values\n",
    "df_test['Week']=date_month.values\n",
    "\n",
    "\n",
    "score = np.zeros((dates.shape[0]),dtype=int)\n",
    "for i in range(dates.shape[0]):\n",
    "    if datetime[i].isoweekday()==6 or datetime[i].isoweekday==7:\n",
    "        score[i]=1\n",
    "    \n",
    "scor=pd.DataFrame(score,columns=['Weekend'])\n",
    "df_test['Weekend']=scor.values\n",
    "\n",
    "\n",
    "score = np.empty((dates.shape[0]),dtype=str)\n",
    "for i in range(dates.shape[0]):\n",
    "    if datetime[i].isoweekday() == 4 and 22 <= datetime[i].day < 29 and datetime[i].month==11:\n",
    "        score[i]='Independance Day'\n",
    "    if 24 <= datetime[i].day <= 25 and datetime[i].month==12:\n",
    "        score[i]='Christmas'\n",
    "    if (datetime[i].day == 31 and datetime[i].month==12) or (datetime[i].day == 1 and datetime[i].month==1):\n",
    "        score[i]='New Year'\n",
    "    \n",
    "scor=pd.DataFrame(score,columns=['Celebs'])\n",
    "df_test['Celebs']=scor.values\n",
    "\n",
    "df_test[['WeeksToDeparture']]= pd.cut(df_test['WeeksToDeparture'],  [0,3.6,4.01,6,8,9,10,11,12,13,14,15,16,17,18.3,20.5,24],labels=[ '0-3', '3-4','4-6','6-8','8-9','9-10','10-11','11-12','12-13','13-14','14-15','15-16','16-17','17-18','18-20','20-24'])\n",
    "\n",
    "cols = df_test.columns.tolist()\n",
    "cols =cols[-4:]+cols[:-4]\n",
    "df_test=df_test[cols]\n",
    "\n",
    "df_test[['LatitudeDeparture']]= min_max_scaler.fit_transform(df_test[['LatitudeDeparture']].values)\n",
    "\n",
    "df_test[['LatitudeArrival']]= min_max_scaler.fit_transform(df_test[['LatitudeArrival']].values)\n",
    "\n",
    "\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Όσον αφορά τις διαστάσεις του test set, όπως βλέπετε παρακάτω, αποτελείται από 2229 γραμμές και 11 στήλες. Η στήλη που λείπει αφορά τη μεταβλητή `PAX` την οποία στόχος σας είναι να προβλέψετε."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2229, 15)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Στόχος\n",
    "\n",
    "Όπως αναφέρθηκε και παραπάνω, στόχος σας είναι να προβλέψετε την κατηγορία αριθμού επιβατών (μεταβλητή `PAX`) για κάθε πτήση του test set. Πρόκειται για ένα supervised learning πρόβλημα. Θα πρέπει να επεξεργαστείτε τα δεδομένα του training set και στη συνέχεια, θα τα χρησιμοποιήσετε για να εκπαιδεύσετε κάποιο ταξινομητή, τον οποίο θα χρησιμοποιήσετε για να κάνετε προβλέψεις σχετικά με τα δεδομένα του test set.\n",
    "\n",
    "\n",
    "### Αξιολόγηση\n",
    "\n",
    "Η αξιολόγηση των προβλέψεων θα γίνει με βάση το `micro F1-score`. Το F1-score παίρνει τιμές μεταξύ 0 και 1. Όσο μεγαλύτερη η τιμή του F1-score, τόσο καλύτερος ο ταξινομητής που παρήγαγε τις προβλέψεις. Για κάθε μια από τις 8 κατηγορίες του test set μας, ο ταξινομητής θα κάνει κάποιες σωστές προβλέψεις και κάποιες λάθος. Πιο συγκεκριμένα, για μια κατηγορία $c$:\n",
    "- τα παραδείγματα που ανήκουν στην κατηγορία $c$ και ο ταξινομητής προέβλεψε ότι πράγματι ανήκουν στην κατηγορία $c$ ονομάζονται True Positives (TP).\n",
    "- τα παραδείγματα που ανήκουν στην κατηγορία $c$ και ο ταξινομητής προέβλεψε λανθασμένως ότι ανήκουν σε κάποια άλλη κατηγορία ονομάζονται False Positives (FP).\n",
    "- τα παραδείγματα που ανήκουν σε κάποια κατηγορία διαφορετική της $c$ και ο ταξινομητής προέβλεψε ότι πράγματι ανήκουν σε κάποια κατηγορία διαφορετική της $c$ ονομάζονται True Negatives (TN).\n",
    "- τα παραδείγματα που ανήκουν σε κατηγορία διαφορετική της $c$ και ο ταξινομητής προέβλεψε ότι ανήκουν στην κατηγορία $c$ ονομάζονται False Negatives (FN).\n",
    "\n",
    "Δεδομένων των παραπάνω ορισμών, μπορούμε συνήθως να υπολογίσουμε μερικές μετρικές απόδοσης του ταξινομητή μας, λαμβάνοντας υπόψη τον συνολικό (για όλες τις κατηγορίες) αριθμό True Positives (TP), False Positives (FP) και False Negatives (FN).\n",
    "Στο πρόβλημα που σας έχει ανατεθεί υπάρχουν συνολικά 8 κατηγορίες $c_1, c_2, \\ldots, c_8$ οπότε μπορούμε να υπολογίσουμε:\n",
    "Πρώτα, το `Recall` το οποίο ορίζεται ως:\n",
    "$$ Recall = \\frac{\\sum_{i=1}^8 TP_{c_i}}{\\sum_{i=1}^8TP_{c_i}+\\sum_{i=1}^8FN_{c_i}} $$\n",
    "και μετράει συνολικά το ποσοστό των παραδειγμάτων που προβλέφτηκαν ότι ανήκουν στην κατηγορία $c_i$ σε σχέση με αυτά που ανήκουν πράγματι στην κατηγορία $c_i$. Έπειτα, το `Precision` το οποίο μας λέει πόσα από τα παραδείγματα που προβλέφτηκαν ότι ανήκουν στην κατηγορία $c_i$, ανήκουν πράγματι σε αυτή την κατηγορία, και ορίζεται ως εξης:\n",
    "$$ Precision = \\frac{\\sum_{i=1}^8 TP_{c_i}}{\\sum_{i=1}^8TP_{c_i}+\\sum_{i=1}^8FP_{c_i}} $$\n",
    "\n",
    "Το `micro F1-score` ειναι ο αρμονικός μέσος του Precision και του Recall, και ορίζεται ως εξής:\n",
    "$$ micro\\text{ } F1-score =  \\frac{2*Recall*Precision}{Recall + Precision} $$\n",
    "Πρόκειται για μια ευρέως διαδεδομένη μετρική απόδοσης η οποία είναι και αξιόπιστη δεδομένου ότι λαμβάνει υπόψη τόσο τα False Positives όσο και τα False Negatives.\n",
    "\n",
    "Αφότου υποβάλετε μια λύση στην πλατφόρμα (όπως περιγράφεται παρακάτω), το micro F1-score της λύσης σας υπολογίζεται αυτόματα και εμφανίζεται στην οθόνη.\n",
    "\n",
    "### Υποβολή Λύσης\n",
    "\n",
    "Αφότου έχετε σχεδιάσει και τρέξει τον αλγοριθμό σας, και έχετε προβλέψει την κατηγορία επιβατών για κάθε πτήση του test set, πρέπει να υποβάλετε την λύση σας στην πλατφόρμα Kaggle ώστε να αξιολογηθεί. Συγκεκριμένα, στην κεντρική οθόνη υπάρχει η επιλογή `Submit Predictions` η οποία σας δίνει τη δυνατότητα να ανεβάσετε τις προβλέψεις σας. Οι προβλέψεις σας θα πρέπει να συμπεριληφθούν σε ένα αρχείο όπου κάθε γραμμή περιέχει την πρόβλεψή σας για την πτήση η οποία βρίσκεται στην ίδια γραμμή του test set. Για παράδειγμα, παρακάτω σας δίνονται οι τρείς πρώτες πτήσεις του test set:\n",
    "``` html\n",
    "2012-10-21,DFW,Dallas-Fort Worth,32.896828,-97.037997,SFO,San Francisco,37.618972,-122.374889,14.6,11.5758369028\n",
    "2012-09-13,LAX,Los Angeles,33.942536,-118.408075,ATL,Atlanta,33.636719,-84.428067,14.7307692308,13.3643037748\n",
    "2012-09-04,ORD,Chicago,41.978603,-87.904842,IAH,Houston,29.984433,-95.341442,8.47058823529,5.88555060146\n",
    "```\n",
    "Η πρώτη γραμμή του αρχείου που θα υποβάλετε θα πρέπει να περιέχει την πρόβλεψή σας για την κατηγορία αριθμού επιβατών της πρώτης πτήσης, η δεύτερη γραμμή θα πρέπει να περιέχει την πρόβλεψή σας για την δεύτερη πτήση και αντίστοιχα η τρίτη γραμμή θα πρέπει να περιέχει την πρόβλεψή σας για την τρίτη πτήση.\n",
    "\n",
    "**Προσοχή**: Στο αρχείο που θα υποβάλετε, κάθε γραμμή θα πρέπει να περιέχει 2 αριθμούς, έναν για το `Id` της πτήσης του test set και έναν για το `Label` που υποδηλώνει την κατηγορία επιβατών (αριθμοί 0-7). Επιπλέον, θα πρέπει να υπάρχουν συνολικά **2229** γραμμές όσες και οι πτήσεις για τις οποίες πρέπει να κάνετε πρόβλεψη. Τέλος, η κωδικοποίηση του αρχείου θα πρέπει να είναι **utf-8**. Φυσικά θα υπάρχει και μία ακόμη γραμμή στην αρχή που περιέχει τό όνομα των πεδίων `Id` και `Label`.\n",
    "\n",
    "### Αρχικός Κώδικας Python\n",
    "\n",
    "Σας δίνεται ένας αρχικός Python κώδικας ο οποίος χρησιμοποιεί ως μόνη πληροφορία το αεροδρόμιο από το οποίο αναχώρησε η πτήση και το αεροδρόμιο στο οποίο προσγειώθηκε για να προβλέψει την κατηγορία αριθμού επιβατών. Συγκεκριμένα, ο παρακάτω κώδικας αποθηκεύει στη μεταβλητή `y_train` τη στήλη `PAX` του training set, δηλαδή την στήλη την οποία θέλουμε να μάθουμε να προβλέπουμε. Στη συνέχεια, διαγράφει όλες τις στήλες του training set εκτός από τις `Departure` και `Arrival` οι οποίες αντιστοιχούν στους κωδικούς αεροδρομίων αναχώρησης και άφιξης. Επίσης για να είναι πιό εύκολος ο έλεγχος της λύσης σας (τοπικά), μπορείτε αν θέλετε να παράγετε ένα νέο training και test set απο το αρχικό training set, χρησιμοποιώντας την  `train_test_split` του `scikit-learn` (σχολιασμένη γραμμή παρακάτω)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lefteris\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  if sys.path[0] == '':\n",
      "C:\\Users\\Lefteris\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n",
      "C:\\Users\\Lefteris\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  app.launch_new_instance()\n",
      "C:\\Users\\Lefteris\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:18: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "C:\\Users\\Lefteris\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "C:\\Users\\Lefteris\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:22: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "C:\\Users\\Lefteris\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:24: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "C:\\Users\\Lefteris\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:26: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "C:\\Users\\Lefteris\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:28: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "C:\\Users\\Lefteris\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:30: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "C:\\Users\\Lefteris\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:32: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Week</th>\n",
       "      <th>Date</th>\n",
       "      <th>Weekend</th>\n",
       "      <th>Celebs</th>\n",
       "      <th>DateOfDeparture</th>\n",
       "      <th>Departure</th>\n",
       "      <th>CityDeparture</th>\n",
       "      <th>LongitudeDeparture</th>\n",
       "      <th>LatitudeDeparture</th>\n",
       "      <th>Arrival</th>\n",
       "      <th>CityArrival</th>\n",
       "      <th>LongitudeArrival</th>\n",
       "      <th>LatitudeArrival</th>\n",
       "      <th>WeeksToDeparture</th>\n",
       "      <th>std_wtd</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Week</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.002901</td>\n",
       "      <td>0.003531</td>\n",
       "      <td>0.055426</td>\n",
       "      <td>-0.133900</td>\n",
       "      <td>0.004729</td>\n",
       "      <td>0.008686</td>\n",
       "      <td>-0.000565</td>\n",
       "      <td>-0.004625</td>\n",
       "      <td>-0.007380</td>\n",
       "      <td>-0.011322</td>\n",
       "      <td>-0.005210</td>\n",
       "      <td>-0.005448</td>\n",
       "      <td>-0.090599</td>\n",
       "      <td>0.085705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <td>0.002901</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.415068</td>\n",
       "      <td>0.000442</td>\n",
       "      <td>-0.009952</td>\n",
       "      <td>0.000550</td>\n",
       "      <td>0.000239</td>\n",
       "      <td>-0.018403</td>\n",
       "      <td>-0.005799</td>\n",
       "      <td>-0.010947</td>\n",
       "      <td>-0.009701</td>\n",
       "      <td>0.009406</td>\n",
       "      <td>0.025082</td>\n",
       "      <td>-0.049651</td>\n",
       "      <td>0.051894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Weekend</th>\n",
       "      <td>0.003531</td>\n",
       "      <td>0.415068</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.029874</td>\n",
       "      <td>0.005279</td>\n",
       "      <td>0.003318</td>\n",
       "      <td>0.002829</td>\n",
       "      <td>0.001502</td>\n",
       "      <td>0.002001</td>\n",
       "      <td>-0.001385</td>\n",
       "      <td>-0.002661</td>\n",
       "      <td>0.012331</td>\n",
       "      <td>0.011434</td>\n",
       "      <td>-0.028205</td>\n",
       "      <td>0.064781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Celebs</th>\n",
       "      <td>0.055426</td>\n",
       "      <td>0.000442</td>\n",
       "      <td>0.029874</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.010886</td>\n",
       "      <td>-0.014054</td>\n",
       "      <td>-0.004745</td>\n",
       "      <td>0.006970</td>\n",
       "      <td>0.006593</td>\n",
       "      <td>0.006694</td>\n",
       "      <td>-0.013059</td>\n",
       "      <td>0.022379</td>\n",
       "      <td>0.011339</td>\n",
       "      <td>0.005645</td>\n",
       "      <td>-0.016370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DateOfDeparture</th>\n",
       "      <td>-0.133900</td>\n",
       "      <td>-0.009952</td>\n",
       "      <td>0.005279</td>\n",
       "      <td>0.010886</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.008173</td>\n",
       "      <td>-0.000654</td>\n",
       "      <td>-0.019295</td>\n",
       "      <td>-0.000211</td>\n",
       "      <td>0.011053</td>\n",
       "      <td>0.009055</td>\n",
       "      <td>-0.002794</td>\n",
       "      <td>-0.001224</td>\n",
       "      <td>0.039095</td>\n",
       "      <td>0.125821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Departure</th>\n",
       "      <td>0.004729</td>\n",
       "      <td>0.000550</td>\n",
       "      <td>0.003318</td>\n",
       "      <td>-0.014054</td>\n",
       "      <td>-0.008173</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.719488</td>\n",
       "      <td>0.257366</td>\n",
       "      <td>-0.333768</td>\n",
       "      <td>-0.124821</td>\n",
       "      <td>-0.096380</td>\n",
       "      <td>0.012543</td>\n",
       "      <td>-0.037847</td>\n",
       "      <td>-0.070353</td>\n",
       "      <td>0.122581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CityDeparture</th>\n",
       "      <td>0.008686</td>\n",
       "      <td>0.000239</td>\n",
       "      <td>0.002829</td>\n",
       "      <td>-0.004745</td>\n",
       "      <td>-0.000654</td>\n",
       "      <td>0.719488</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.104746</td>\n",
       "      <td>-0.202584</td>\n",
       "      <td>-0.090325</td>\n",
       "      <td>-0.136966</td>\n",
       "      <td>-0.042263</td>\n",
       "      <td>-0.041017</td>\n",
       "      <td>-0.050082</td>\n",
       "      <td>0.132701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LongitudeDeparture</th>\n",
       "      <td>-0.000565</td>\n",
       "      <td>-0.018403</td>\n",
       "      <td>0.001502</td>\n",
       "      <td>0.006970</td>\n",
       "      <td>-0.019295</td>\n",
       "      <td>0.257366</td>\n",
       "      <td>0.104746</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.289339</td>\n",
       "      <td>0.052148</td>\n",
       "      <td>-0.008896</td>\n",
       "      <td>-0.036443</td>\n",
       "      <td>-0.127279</td>\n",
       "      <td>0.075330</td>\n",
       "      <td>-0.142707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LatitudeDeparture</th>\n",
       "      <td>-0.004625</td>\n",
       "      <td>-0.005799</td>\n",
       "      <td>0.002001</td>\n",
       "      <td>0.006593</td>\n",
       "      <td>-0.000211</td>\n",
       "      <td>-0.333768</td>\n",
       "      <td>-0.202584</td>\n",
       "      <td>0.289339</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.000298</td>\n",
       "      <td>0.002322</td>\n",
       "      <td>-0.121348</td>\n",
       "      <td>0.183342</td>\n",
       "      <td>0.114671</td>\n",
       "      <td>-0.135811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Arrival</th>\n",
       "      <td>-0.007380</td>\n",
       "      <td>-0.010947</td>\n",
       "      <td>-0.001385</td>\n",
       "      <td>0.006694</td>\n",
       "      <td>0.011053</td>\n",
       "      <td>-0.124821</td>\n",
       "      <td>-0.090325</td>\n",
       "      <td>0.052148</td>\n",
       "      <td>-0.000298</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.713527</td>\n",
       "      <td>0.224927</td>\n",
       "      <td>-0.351012</td>\n",
       "      <td>-0.096136</td>\n",
       "      <td>0.118909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CityArrival</th>\n",
       "      <td>-0.011322</td>\n",
       "      <td>-0.009701</td>\n",
       "      <td>-0.002661</td>\n",
       "      <td>-0.013059</td>\n",
       "      <td>0.009055</td>\n",
       "      <td>-0.096380</td>\n",
       "      <td>-0.136966</td>\n",
       "      <td>-0.008896</td>\n",
       "      <td>0.002322</td>\n",
       "      <td>0.713527</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.044968</td>\n",
       "      <td>-0.229197</td>\n",
       "      <td>-0.082103</td>\n",
       "      <td>0.147669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LongitudeArrival</th>\n",
       "      <td>-0.005210</td>\n",
       "      <td>0.009406</td>\n",
       "      <td>0.012331</td>\n",
       "      <td>0.022379</td>\n",
       "      <td>-0.002794</td>\n",
       "      <td>0.012543</td>\n",
       "      <td>-0.042263</td>\n",
       "      <td>-0.036443</td>\n",
       "      <td>-0.121348</td>\n",
       "      <td>0.224927</td>\n",
       "      <td>0.044968</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.295282</td>\n",
       "      <td>0.065503</td>\n",
       "      <td>-0.146647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LatitudeArrival</th>\n",
       "      <td>-0.005448</td>\n",
       "      <td>0.025082</td>\n",
       "      <td>0.011434</td>\n",
       "      <td>0.011339</td>\n",
       "      <td>-0.001224</td>\n",
       "      <td>-0.037847</td>\n",
       "      <td>-0.041017</td>\n",
       "      <td>-0.127279</td>\n",
       "      <td>0.183342</td>\n",
       "      <td>-0.351012</td>\n",
       "      <td>-0.229197</td>\n",
       "      <td>0.295282</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.131651</td>\n",
       "      <td>-0.135959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>WeeksToDeparture</th>\n",
       "      <td>-0.090599</td>\n",
       "      <td>-0.049651</td>\n",
       "      <td>-0.028205</td>\n",
       "      <td>0.005645</td>\n",
       "      <td>0.039095</td>\n",
       "      <td>-0.070353</td>\n",
       "      <td>-0.050082</td>\n",
       "      <td>0.075330</td>\n",
       "      <td>0.114671</td>\n",
       "      <td>-0.096136</td>\n",
       "      <td>-0.082103</td>\n",
       "      <td>0.065503</td>\n",
       "      <td>0.131651</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.457659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std_wtd</th>\n",
       "      <td>0.085705</td>\n",
       "      <td>0.051894</td>\n",
       "      <td>0.064781</td>\n",
       "      <td>-0.016370</td>\n",
       "      <td>0.125821</td>\n",
       "      <td>0.122581</td>\n",
       "      <td>0.132701</td>\n",
       "      <td>-0.142707</td>\n",
       "      <td>-0.135811</td>\n",
       "      <td>0.118909</td>\n",
       "      <td>0.147669</td>\n",
       "      <td>-0.146647</td>\n",
       "      <td>-0.135959</td>\n",
       "      <td>-0.457659</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        Week      Date   Weekend    Celebs  DateOfDeparture  \\\n",
       "Week                1.000000  0.002901  0.003531  0.055426        -0.133900   \n",
       "Date                0.002901  1.000000  0.415068  0.000442        -0.009952   \n",
       "Weekend             0.003531  0.415068  1.000000  0.029874         0.005279   \n",
       "Celebs              0.055426  0.000442  0.029874  1.000000         0.010886   \n",
       "DateOfDeparture    -0.133900 -0.009952  0.005279  0.010886         1.000000   \n",
       "Departure           0.004729  0.000550  0.003318 -0.014054        -0.008173   \n",
       "CityDeparture       0.008686  0.000239  0.002829 -0.004745        -0.000654   \n",
       "LongitudeDeparture -0.000565 -0.018403  0.001502  0.006970        -0.019295   \n",
       "LatitudeDeparture  -0.004625 -0.005799  0.002001  0.006593        -0.000211   \n",
       "Arrival            -0.007380 -0.010947 -0.001385  0.006694         0.011053   \n",
       "CityArrival        -0.011322 -0.009701 -0.002661 -0.013059         0.009055   \n",
       "LongitudeArrival   -0.005210  0.009406  0.012331  0.022379        -0.002794   \n",
       "LatitudeArrival    -0.005448  0.025082  0.011434  0.011339        -0.001224   \n",
       "WeeksToDeparture   -0.090599 -0.049651 -0.028205  0.005645         0.039095   \n",
       "std_wtd             0.085705  0.051894  0.064781 -0.016370         0.125821   \n",
       "\n",
       "                    Departure  CityDeparture  LongitudeDeparture  \\\n",
       "Week                 0.004729       0.008686           -0.000565   \n",
       "Date                 0.000550       0.000239           -0.018403   \n",
       "Weekend              0.003318       0.002829            0.001502   \n",
       "Celebs              -0.014054      -0.004745            0.006970   \n",
       "DateOfDeparture     -0.008173      -0.000654           -0.019295   \n",
       "Departure            1.000000       0.719488            0.257366   \n",
       "CityDeparture        0.719488       1.000000            0.104746   \n",
       "LongitudeDeparture   0.257366       0.104746            1.000000   \n",
       "LatitudeDeparture   -0.333768      -0.202584            0.289339   \n",
       "Arrival             -0.124821      -0.090325            0.052148   \n",
       "CityArrival         -0.096380      -0.136966           -0.008896   \n",
       "LongitudeArrival     0.012543      -0.042263           -0.036443   \n",
       "LatitudeArrival     -0.037847      -0.041017           -0.127279   \n",
       "WeeksToDeparture    -0.070353      -0.050082            0.075330   \n",
       "std_wtd              0.122581       0.132701           -0.142707   \n",
       "\n",
       "                    LatitudeDeparture   Arrival  CityArrival  \\\n",
       "Week                        -0.004625 -0.007380    -0.011322   \n",
       "Date                        -0.005799 -0.010947    -0.009701   \n",
       "Weekend                      0.002001 -0.001385    -0.002661   \n",
       "Celebs                       0.006593  0.006694    -0.013059   \n",
       "DateOfDeparture             -0.000211  0.011053     0.009055   \n",
       "Departure                   -0.333768 -0.124821    -0.096380   \n",
       "CityDeparture               -0.202584 -0.090325    -0.136966   \n",
       "LongitudeDeparture           0.289339  0.052148    -0.008896   \n",
       "LatitudeDeparture            1.000000 -0.000298     0.002322   \n",
       "Arrival                     -0.000298  1.000000     0.713527   \n",
       "CityArrival                  0.002322  0.713527     1.000000   \n",
       "LongitudeArrival            -0.121348  0.224927     0.044968   \n",
       "LatitudeArrival              0.183342 -0.351012    -0.229197   \n",
       "WeeksToDeparture             0.114671 -0.096136    -0.082103   \n",
       "std_wtd                     -0.135811  0.118909     0.147669   \n",
       "\n",
       "                    LongitudeArrival  LatitudeArrival  WeeksToDeparture  \\\n",
       "Week                       -0.005210        -0.005448         -0.090599   \n",
       "Date                        0.009406         0.025082         -0.049651   \n",
       "Weekend                     0.012331         0.011434         -0.028205   \n",
       "Celebs                      0.022379         0.011339          0.005645   \n",
       "DateOfDeparture            -0.002794        -0.001224          0.039095   \n",
       "Departure                   0.012543        -0.037847         -0.070353   \n",
       "CityDeparture              -0.042263        -0.041017         -0.050082   \n",
       "LongitudeDeparture         -0.036443        -0.127279          0.075330   \n",
       "LatitudeDeparture          -0.121348         0.183342          0.114671   \n",
       "Arrival                     0.224927        -0.351012         -0.096136   \n",
       "CityArrival                 0.044968        -0.229197         -0.082103   \n",
       "LongitudeArrival            1.000000         0.295282          0.065503   \n",
       "LatitudeArrival             0.295282         1.000000          0.131651   \n",
       "WeeksToDeparture            0.065503         0.131651          1.000000   \n",
       "std_wtd                    -0.146647        -0.135959         -0.457659   \n",
       "\n",
       "                     std_wtd  \n",
       "Week                0.085705  \n",
       "Date                0.051894  \n",
       "Weekend             0.064781  \n",
       "Celebs             -0.016370  \n",
       "DateOfDeparture     0.125821  \n",
       "Departure           0.122581  \n",
       "CityDeparture       0.132701  \n",
       "LongitudeDeparture -0.142707  \n",
       "LatitudeDeparture  -0.135811  \n",
       "Arrival             0.118909  \n",
       "CityArrival         0.147669  \n",
       "LongitudeArrival   -0.146647  \n",
       "LatitudeArrival    -0.135959  \n",
       "WeeksToDeparture   -0.457659  \n",
       "std_wtd             1.000000  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# load data\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "X = df_train[['Week','Date','Weekend','Celebs','DateOfDeparture','Departure','CityDeparture','LongitudeDeparture','LatitudeDeparture','Arrival','CityArrival','LongitudeArrival','LatitudeArrival','WeeksToDeparture','std_wtd']]\n",
    "Y = df_train[['PAX']]\n",
    "\n",
    "\n",
    "le = LabelEncoder()\n",
    "le.fit(X['WeeksToDeparture'])\n",
    "X['WeeksToDeparture'] = le.transform(X['WeeksToDeparture'])\n",
    "le.fit(X['DateOfDeparture'])\n",
    "X['DateOfDeparture'] = le.transform(X['DateOfDeparture'])\n",
    "le.fit(X['Celebs'])\n",
    "X['Celebs'] = le.transform(X['Celebs'])\n",
    "le.fit(X['Departure'])\n",
    "X['Departure'] = le.transform(X['Departure'])\n",
    "le.fit(X['CityDeparture'])\n",
    "X['CityDeparture'] = le.transform(X['CityDeparture'])\n",
    "le.fit(X['LongitudeDeparture'])\n",
    "X['LongitudeDeparture'] = le.transform(X['LongitudeDeparture'])\n",
    "le.fit(X['LatitudeDeparture'])\n",
    "X['LatitudeDeparture'] = le.transform(X['LatitudeDeparture'])\n",
    "le.fit(X['Arrival'])\n",
    "X['Arrival'] = le.transform(X['Arrival'])\n",
    "le.fit(X['CityArrival'])\n",
    "X['CityArrival'] = le.transform(X['CityArrival'])\n",
    "le.fit(X['LongitudeArrival'])\n",
    "X['LongitudeArrival'] = le.transform(X['LongitudeArrival'])\n",
    "le.fit(X['LatitudeArrival'])\n",
    "X['LatitudeArrival'] = le.transform(X['LatitudeArrival'])\n",
    "X.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   93.401   740.159  1842.208  1792.917 11517.645   499.986   344.961\n",
      "   292.807   502.604   365.252   343.649   296.733   445.855   401.518\n",
      "   115.002]\n",
      "[[  1.   0.   0.  95.]\n",
      " [  5.   0.   0. 547.]\n",
      " [  1.   0.   0. 158.]\n",
      " [  1.   0.   0. 466.]\n",
      " [  5.   0.   0. 225.]]\n"
     ]
    }
   ],
   "source": [
    "import pandas\n",
    "import numpy\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "# feature extraction\n",
    "test = SelectKBest(score_func=chi2, k=4)\n",
    "fit = test.fit(X, Y)\n",
    "# summarize scores\n",
    "numpy.set_printoptions(precision=3)\n",
    "print(fit.scores_)\n",
    "features = fit.transform(X)\n",
    "# summarize selected features\n",
    "print(features[0:5,:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.147 0.109 0.035 0.011 0.152 0.036 0.034 0.034 0.034 0.038 0.04  0.042\n",
      " 0.038 0.107 0.143]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lefteris\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "C:\\Users\\Lefteris\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:6: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "\n",
    "# feature extraction\n",
    "model = ExtraTreesClassifier()\n",
    "model.fit(X, Y)\n",
    "print(model.feature_importances_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lefteris\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:761: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Lefteris\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Lefteris\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n",
      "C:\\Users\\Lefteris\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Lefteris\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n",
      "C:\\Users\\Lefteris\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Lefteris\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n",
      "C:\\Users\\Lefteris\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Lefteris\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n",
      "C:\\Users\\Lefteris\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Lefteris\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n",
      "C:\\Users\\Lefteris\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Lefteris\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n",
      "C:\\Users\\Lefteris\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Lefteris\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n",
      "C:\\Users\\Lefteris\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Lefteris\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n",
      "C:\\Users\\Lefteris\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Lefteris\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n",
      "C:\\Users\\Lefteris\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Lefteris\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n",
      "C:\\Users\\Lefteris\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Lefteris\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "[False  True  True  True False False False False False False False False\n",
      " False False  True]\n",
      "[11  1  1  1 12  2  3  9  7  5  4  8  6 10  1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lefteris\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Lefteris\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "# load data\n",
    "\n",
    "# feature extraction\n",
    "model = LogisticRegression()\n",
    "rfe = RFE(model, 4)\n",
    "fit = rfe.fit(X, Y)\n",
    "print(fit.n_features_) \n",
    "print(fit.support_)  \n",
    "print(fit.ranking_)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Week</th>\n",
       "      <th>Weekend</th>\n",
       "      <th>Celebs</th>\n",
       "      <th>Departure</th>\n",
       "      <th>Arrival</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8482</th>\n",
       "      <td>6</td>\n",
       "      <td>36</td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td>DFW</td>\n",
       "      <td>ATL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7687</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td>LAX</td>\n",
       "      <td>ORD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5923</th>\n",
       "      <td>1</td>\n",
       "      <td>43</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td>ORD</td>\n",
       "      <td>DEN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6661</th>\n",
       "      <td>5</td>\n",
       "      <td>46</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td>CLT</td>\n",
       "      <td>ATL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6891</th>\n",
       "      <td>2</td>\n",
       "      <td>47</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td>DFW</td>\n",
       "      <td>LAS</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Date  Week  Weekend Celebs Departure Arrival\n",
       "8482     6    36        1              DFW     ATL\n",
       "7687     1     4        0              LAX     ORD\n",
       "5923     1    43        0              ORD     DEN\n",
       "6661     5    46        0              CLT     ATL\n",
       "6891     2    47        0              DFW     LAS"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train = df_train[['PAX']]\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "df_train, df_test, y_train, y_test = train_test_split(df_train, y_train, test_size=0.2, random_state=42,stratify=y_train)\n",
    "\n",
    "df_train.drop(df_train.columns[[4,6,7,8,10,11,12,13,14,15]], axis=1, inplace=True)\n",
    "\n",
    "df_train.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Για να μπορεί να δουλέψει ένα αλγόριθμος ταξινόμησης είναι απαραίτητο το training και το test set να έχουν ακριβώς τον ίδιο αριθμό στηλών (ίδιες μεταβλητές). Συνεπώς, πρέπει και στο test set να διαγράψουμε όλες τις στήλες εκτός από τις Departure και Arrival. Αυτό γίνεται με τον παρακάτω κώδικα."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Week</th>\n",
       "      <th>Weekend</th>\n",
       "      <th>Celebs</th>\n",
       "      <th>Departure</th>\n",
       "      <th>Arrival</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2231</th>\n",
       "      <td>6</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td>BOS</td>\n",
       "      <td>PHL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3837</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td>BOS</td>\n",
       "      <td>PHL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7812</th>\n",
       "      <td>4</td>\n",
       "      <td>40</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td>ATL</td>\n",
       "      <td>CLT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6721</th>\n",
       "      <td>2</td>\n",
       "      <td>50</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td>MCO</td>\n",
       "      <td>ATL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>5</td>\n",
       "      <td>39</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td>LAX</td>\n",
       "      <td>ORD</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Date  Week  Weekend Celebs Departure Arrival\n",
       "2231     6    14        1              BOS     PHL\n",
       "3837     5     1        0              BOS     PHL\n",
       "7812     4    40        0              ATL     CLT\n",
       "6721     2    50        0              MCO     ATL\n",
       "8        5    39        0              LAX     ORD"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.drop(df_test.columns[[4,6,7,8,10,11,12,13,14,15]], axis=1, inplace=True)\n",
    "\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ένας τρόπος αναπαράστασης κατηγορικών μεταβλητών είναι το λεγόμενο one-hot encoding όπου υπάρχει διαθέσιμη μια μεταβλητή για κάθε πιθανή τιμή του χαρακτηριστικού και ανάλογα με την τρέχουσα τιμή του, μια από αυτές της μεταβλητές είναι 1, ενώ όλες οι άλλες παραμένουν 0. Για παράδειγμα, αν είχαμε κάποια μεταβλητή Weekday η οποία περιέγραφε τη μέρα που έγινε μια πτήση, θα είχαμε 7 μεταβλητές (π.χ. 1000000 για Monday, 0100000 για Tuesday κτλ.). Σημειώστε ότι με την one-hot encoding αναπαράσταση ο αριθμός των χαρακτηριστικών που προκύπτει είναι ίσος με τον αριθμό των διαφορετικών τιμών που παίρνει η μεταβλητή. Παρακάτω εφαρμόζουμε one-hot encoding στις μεταβλητές `Departure` και `Arrival`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7119, 105)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "X_train = df_train\n",
    "X_test = df_test\n",
    "y_train = np.ravel(y_train)\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "enc = OneHotEncoder(sparse=False,categories='auto')\n",
    "enc.fit(df_train)  \n",
    "X_train = enc.transform(df_train)\n",
    "X_test = enc.transform(df_test)\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Βλέπουμε ότι ο αριθμός των στηλών αυξήθηκε από 2 σε 40. Αυτό συνέβη γιατί υπάρχουν 20 διαφορετικά αεροδρόμια και συνεπώς χρειαζόμαστε 20 μεταβλητές για να αναπαραστήσουμε κάθε μια από τις μεταβλητές `Departure` και `Arrival` χρησιμοποιώντας one-hot encoding.\n",
    "\n",
    "Έπειτα, εκπαιδεύουμε ξανά έναν ταξινομητή logistic regression για να προβλέψουμε τις κατηγορίες αριθμού επιβατών των δεδομένων ελέγχου. Επιπλέον, αποθηκεύουμε τις προβλέψεις μας στο αρχείο `y_pred.csv` στο δίσκο."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 2.12736485\n",
      "Iteration 2, loss = 1.92531943\n",
      "Iteration 3, loss = 1.86394095\n",
      "Iteration 4, loss = 1.83653446\n",
      "Iteration 5, loss = 1.81918955\n",
      "Iteration 6, loss = 1.80601367\n",
      "Iteration 7, loss = 1.79487896\n",
      "Iteration 8, loss = 1.78468262\n",
      "Iteration 9, loss = 1.77487099\n",
      "Iteration 10, loss = 1.76517459\n",
      "Iteration 11, loss = 1.75507490\n",
      "Iteration 12, loss = 1.74489206\n",
      "Iteration 13, loss = 1.73408774\n",
      "Iteration 14, loss = 1.72262050\n",
      "Iteration 15, loss = 1.71087017\n",
      "Iteration 16, loss = 1.69643892\n",
      "Iteration 17, loss = 1.68098010\n",
      "Iteration 18, loss = 1.66401023\n",
      "Iteration 19, loss = 1.64479198\n",
      "Iteration 20, loss = 1.62293531\n",
      "Iteration 21, loss = 1.60008155\n",
      "Iteration 22, loss = 1.57784608\n",
      "Iteration 23, loss = 1.55322292\n",
      "Iteration 24, loss = 1.53151351\n",
      "Iteration 25, loss = 1.50931799\n",
      "Iteration 26, loss = 1.48827240\n",
      "Iteration 27, loss = 1.47040850\n",
      "Iteration 28, loss = 1.45207993\n",
      "Iteration 29, loss = 1.43772787\n",
      "Iteration 30, loss = 1.42266740\n",
      "Iteration 31, loss = 1.41009328\n",
      "Iteration 32, loss = 1.39946674\n",
      "Iteration 33, loss = 1.38879723\n",
      "Iteration 34, loss = 1.37948257\n",
      "Iteration 35, loss = 1.37131475\n",
      "Iteration 36, loss = 1.36355523\n",
      "Iteration 37, loss = 1.35626599\n",
      "Iteration 38, loss = 1.34975984\n",
      "Iteration 39, loss = 1.34301502\n",
      "Iteration 40, loss = 1.33711653\n",
      "Iteration 41, loss = 1.33094891\n",
      "Iteration 42, loss = 1.32504950\n",
      "Iteration 43, loss = 1.32028927\n",
      "Iteration 44, loss = 1.31437968\n",
      "Iteration 45, loss = 1.30949054\n",
      "Iteration 46, loss = 1.30463431\n",
      "Iteration 47, loss = 1.29688937\n",
      "Iteration 48, loss = 1.29094281\n",
      "Iteration 49, loss = 1.28623341\n",
      "Iteration 50, loss = 1.27992621\n",
      "Iteration 51, loss = 1.27319973\n",
      "Iteration 52, loss = 1.26848443\n",
      "Iteration 53, loss = 1.26118551\n",
      "Iteration 54, loss = 1.25516951\n",
      "Iteration 55, loss = 1.24871659\n",
      "Iteration 56, loss = 1.24355306\n",
      "Iteration 57, loss = 1.23620751\n",
      "Iteration 58, loss = 1.23123455\n",
      "Iteration 59, loss = 1.22374913\n",
      "Iteration 60, loss = 1.21670373\n",
      "Iteration 61, loss = 1.21205663\n",
      "Iteration 62, loss = 1.20610506\n",
      "Iteration 63, loss = 1.20063156\n",
      "Iteration 64, loss = 1.19442648\n",
      "Iteration 65, loss = 1.18961697\n",
      "Iteration 66, loss = 1.18529475\n",
      "Iteration 67, loss = 1.17945998\n",
      "Iteration 68, loss = 1.17449803\n",
      "Iteration 69, loss = 1.17159624\n",
      "Iteration 70, loss = 1.16536385\n",
      "Iteration 71, loss = 1.16035942\n",
      "Iteration 72, loss = 1.15546432\n",
      "Iteration 73, loss = 1.15094804\n",
      "Iteration 74, loss = 1.14644125\n",
      "Iteration 75, loss = 1.14212467\n",
      "Iteration 76, loss = 1.13829606\n",
      "Iteration 77, loss = 1.13542561\n",
      "Iteration 78, loss = 1.13047096\n",
      "Iteration 79, loss = 1.12706591\n",
      "Iteration 80, loss = 1.12165952\n",
      "Iteration 81, loss = 1.11970510\n",
      "Iteration 82, loss = 1.11442621\n",
      "Iteration 83, loss = 1.11121636\n",
      "Iteration 84, loss = 1.10685843\n",
      "Iteration 85, loss = 1.10512628\n",
      "Iteration 86, loss = 1.09981638\n",
      "Iteration 87, loss = 1.09702902\n",
      "Iteration 88, loss = 1.09472623\n",
      "Iteration 89, loss = 1.09133967\n",
      "Iteration 90, loss = 1.08690224\n",
      "Iteration 91, loss = 1.08509570\n",
      "Iteration 92, loss = 1.08177013\n",
      "Iteration 93, loss = 1.07961002\n",
      "Iteration 94, loss = 1.07602035\n",
      "Iteration 95, loss = 1.07337776\n",
      "Iteration 96, loss = 1.07129283\n",
      "Iteration 97, loss = 1.06852878\n",
      "Iteration 98, loss = 1.06641327\n",
      "Iteration 99, loss = 1.06343145\n",
      "Iteration 100, loss = 1.06143200\n",
      "Iteration 101, loss = 1.05811939\n",
      "Iteration 102, loss = 1.05766795\n",
      "Iteration 103, loss = 1.05321295\n",
      "Iteration 104, loss = 1.05181960\n",
      "Iteration 105, loss = 1.04912676\n",
      "Iteration 106, loss = 1.04717801\n",
      "Iteration 107, loss = 1.04454720\n",
      "Iteration 108, loss = 1.04265574\n",
      "Iteration 109, loss = 1.04036737\n",
      "Iteration 110, loss = 1.03837880\n",
      "Iteration 111, loss = 1.03673999\n",
      "Iteration 112, loss = 1.03469173\n",
      "Iteration 113, loss = 1.03071699\n",
      "Iteration 114, loss = 1.02918237\n",
      "Iteration 115, loss = 1.02774375\n",
      "Iteration 116, loss = 1.02663365\n",
      "Iteration 117, loss = 1.02448026\n",
      "Iteration 118, loss = 1.02192397\n",
      "Iteration 119, loss = 1.02034672\n",
      "Iteration 120, loss = 1.01947683\n",
      "Iteration 121, loss = 1.01709583\n",
      "Iteration 122, loss = 1.01557312\n",
      "Iteration 123, loss = 1.01353634\n",
      "Iteration 124, loss = 1.01120894\n",
      "Iteration 125, loss = 1.00913863\n",
      "Iteration 126, loss = 1.00810526\n",
      "Iteration 127, loss = 1.00645543\n",
      "Iteration 128, loss = 1.00504774\n",
      "Iteration 129, loss = 1.00313851\n",
      "Iteration 130, loss = 1.00041033\n",
      "Iteration 131, loss = 0.99832579\n",
      "Iteration 132, loss = 0.99663227\n",
      "Iteration 133, loss = 0.99538888\n",
      "Iteration 134, loss = 0.99352284\n",
      "Iteration 135, loss = 0.99186340\n",
      "Iteration 136, loss = 0.99106222\n",
      "Iteration 137, loss = 0.98875385\n",
      "Iteration 138, loss = 0.98745580\n",
      "Iteration 139, loss = 0.98407475\n",
      "Iteration 140, loss = 0.98519763\n",
      "Iteration 141, loss = 0.98336613\n",
      "Iteration 142, loss = 0.98191957\n",
      "Iteration 143, loss = 0.97906963\n",
      "Iteration 144, loss = 0.97825713\n",
      "Iteration 145, loss = 0.97564520\n",
      "Iteration 146, loss = 0.97447322\n",
      "Iteration 147, loss = 0.97525885\n",
      "Iteration 148, loss = 0.97191418\n",
      "Iteration 149, loss = 0.97108776\n",
      "Iteration 150, loss = 0.96972005\n",
      "Iteration 151, loss = 0.96811278\n",
      "Iteration 152, loss = 0.96759824\n",
      "Iteration 153, loss = 0.96725506\n",
      "Iteration 154, loss = 0.96568519\n",
      "Iteration 155, loss = 0.96546587\n",
      "Iteration 156, loss = 0.96215463\n",
      "Iteration 157, loss = 0.96088921\n",
      "Iteration 158, loss = 0.96133431\n",
      "Iteration 159, loss = 0.95884257\n",
      "Iteration 160, loss = 0.95825732\n",
      "Iteration 161, loss = 0.95619533\n",
      "Iteration 162, loss = 0.95443225\n",
      "Iteration 163, loss = 0.95333698\n",
      "Iteration 164, loss = 0.95216559\n",
      "Iteration 165, loss = 0.95098318\n",
      "Iteration 166, loss = 0.95097220\n",
      "Iteration 167, loss = 0.94845172\n",
      "Iteration 168, loss = 0.94961685\n",
      "Iteration 169, loss = 0.94580946\n",
      "Iteration 170, loss = 0.94487741\n",
      "Iteration 171, loss = 0.94395421\n",
      "Iteration 172, loss = 0.94347627\n",
      "Iteration 173, loss = 0.94179329\n",
      "Iteration 174, loss = 0.94313281\n",
      "Iteration 175, loss = 0.94174947\n",
      "Iteration 176, loss = 0.93897654\n",
      "Iteration 177, loss = 0.93895172\n",
      "Iteration 178, loss = 0.93824622\n",
      "Iteration 179, loss = 0.93715219\n",
      "Iteration 180, loss = 0.93729876\n",
      "Iteration 181, loss = 0.93266657\n",
      "Iteration 182, loss = 0.93282149\n",
      "Iteration 183, loss = 0.93248777\n",
      "Iteration 184, loss = 0.93046620\n",
      "Iteration 185, loss = 0.93040404\n",
      "Iteration 186, loss = 0.93002973\n",
      "Iteration 187, loss = 0.92826432\n",
      "Iteration 188, loss = 0.92863833\n",
      "Iteration 189, loss = 0.92584919\n",
      "Iteration 190, loss = 0.92562175\n",
      "Iteration 191, loss = 0.92544450\n",
      "Iteration 192, loss = 0.92510583\n",
      "Iteration 193, loss = 0.92137038\n",
      "Iteration 194, loss = 0.92286891\n",
      "Iteration 195, loss = 0.92023415\n",
      "Iteration 196, loss = 0.92081609\n",
      "Iteration 197, loss = 0.92083016\n",
      "Iteration 198, loss = 0.91767784\n",
      "Iteration 199, loss = 0.91715224\n",
      "Iteration 200, loss = 0.92004435\n",
      "Iteration 201, loss = 0.91436441\n",
      "Iteration 202, loss = 0.91662798\n",
      "Iteration 203, loss = 0.91380821\n",
      "Iteration 204, loss = 0.91305298\n",
      "Iteration 205, loss = 0.91506098\n",
      "Iteration 206, loss = 0.91060333\n",
      "Iteration 207, loss = 0.90929803\n",
      "Iteration 208, loss = 0.91028547\n",
      "Iteration 209, loss = 0.90749714\n",
      "Iteration 210, loss = 0.90858317\n",
      "Iteration 211, loss = 0.90686203\n",
      "Iteration 212, loss = 0.90585924\n",
      "Iteration 213, loss = 0.90430680\n",
      "Iteration 214, loss = 0.90577681\n",
      "Iteration 215, loss = 0.90381345\n",
      "Iteration 216, loss = 0.90309929\n",
      "Iteration 217, loss = 0.90106689\n",
      "Iteration 218, loss = 0.90064021\n",
      "Iteration 219, loss = 0.89913893\n",
      "Iteration 220, loss = 0.89783683\n",
      "Iteration 221, loss = 0.89915546\n",
      "Iteration 222, loss = 0.89708014\n",
      "Iteration 223, loss = 0.89674736\n",
      "Iteration 224, loss = 0.89516902\n",
      "Iteration 225, loss = 0.89594024\n",
      "Iteration 226, loss = 0.89613307\n",
      "Iteration 227, loss = 0.89453618\n",
      "Iteration 228, loss = 0.89269453\n",
      "Iteration 229, loss = 0.89317259\n",
      "Iteration 230, loss = 0.89262142\n",
      "Iteration 231, loss = 0.89214239\n",
      "Iteration 232, loss = 0.89259067\n",
      "Iteration 233, loss = 0.89038360\n",
      "Iteration 234, loss = 0.89076905\n",
      "Iteration 235, loss = 0.88879892\n",
      "Iteration 236, loss = 0.88754351\n",
      "Iteration 237, loss = 0.88842873\n",
      "Iteration 238, loss = 0.88463475\n",
      "Iteration 239, loss = 0.88460130\n",
      "Iteration 240, loss = 0.88533750\n",
      "Iteration 241, loss = 0.88310524\n",
      "Iteration 242, loss = 0.88531236\n",
      "Iteration 243, loss = 0.88364680\n",
      "Iteration 244, loss = 0.88504959\n",
      "Iteration 245, loss = 0.88301772\n",
      "Iteration 246, loss = 0.88117304\n",
      "Iteration 247, loss = 0.88070744\n",
      "Iteration 248, loss = 0.88308670\n",
      "Iteration 249, loss = 0.88048032\n",
      "Iteration 250, loss = 0.87937337\n",
      "Iteration 251, loss = 0.88013986\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 252, loss = 0.87837237\n",
      "Iteration 253, loss = 0.87833249\n",
      "Iteration 254, loss = 0.87763374\n",
      "Iteration 255, loss = 0.87477984\n",
      "Iteration 256, loss = 0.87501736\n",
      "Iteration 257, loss = 0.87630151\n",
      "Iteration 258, loss = 0.87389412\n",
      "Iteration 259, loss = 0.87499262\n",
      "Iteration 260, loss = 0.87250307\n",
      "Iteration 261, loss = 0.87511369\n",
      "Iteration 262, loss = 0.87084267\n",
      "Iteration 263, loss = 0.87017150\n",
      "Iteration 264, loss = 0.87171089\n",
      "Iteration 265, loss = 0.87082058\n",
      "Iteration 266, loss = 0.86986883\n",
      "Iteration 267, loss = 0.87219652\n",
      "Iteration 268, loss = 0.87253369\n",
      "Iteration 269, loss = 0.87000477\n",
      "Iteration 270, loss = 0.86901982\n",
      "Iteration 271, loss = 0.87036109\n",
      "Iteration 272, loss = 0.86688526\n",
      "Iteration 273, loss = 0.86710417\n",
      "Iteration 274, loss = 0.86766066\n",
      "Iteration 275, loss = 0.86597982\n",
      "Iteration 276, loss = 0.86403692\n",
      "Iteration 277, loss = 0.86642167\n",
      "Iteration 278, loss = 0.86482217\n",
      "Iteration 279, loss = 0.86445144\n",
      "Iteration 280, loss = 0.86276691\n",
      "Iteration 281, loss = 0.86230807\n",
      "Iteration 282, loss = 0.86165482\n",
      "Iteration 283, loss = 0.86425271\n",
      "Iteration 284, loss = 0.86195353\n",
      "Iteration 285, loss = 0.85937637\n",
      "Iteration 286, loss = 0.86038169\n",
      "Iteration 287, loss = 0.85777043\n",
      "Iteration 288, loss = 0.86029273\n",
      "Iteration 289, loss = 0.86180235\n",
      "Iteration 290, loss = 0.85945474\n",
      "Iteration 291, loss = 0.85797338\n",
      "Iteration 292, loss = 0.86184000\n",
      "Iteration 293, loss = 0.85736345\n",
      "Iteration 294, loss = 0.85784662\n",
      "Iteration 295, loss = 0.85716437\n",
      "Iteration 296, loss = 0.85755108\n",
      "Iteration 297, loss = 0.85511241\n",
      "Iteration 298, loss = 0.85656055\n",
      "Iteration 299, loss = 0.85822077\n",
      "Iteration 300, loss = 0.85300455\n",
      "Iteration 301, loss = 0.85687123\n",
      "Iteration 302, loss = 0.85771031\n",
      "Iteration 303, loss = 0.85249894\n",
      "Iteration 304, loss = 0.85480730\n",
      "Iteration 305, loss = 0.85360719\n",
      "Iteration 306, loss = 0.85571811\n",
      "Iteration 307, loss = 0.85624082\n",
      "Iteration 308, loss = 0.85350708\n",
      "Iteration 309, loss = 0.85203685\n",
      "Iteration 310, loss = 0.85057763\n",
      "Iteration 311, loss = 0.85071999\n",
      "Iteration 312, loss = 0.85357214\n",
      "Iteration 313, loss = 0.85026212\n",
      "Iteration 314, loss = 0.85123899\n",
      "Iteration 315, loss = 0.85178379\n",
      "Iteration 316, loss = 0.84795219\n",
      "Iteration 317, loss = 0.84794930\n",
      "Iteration 318, loss = 0.84889021\n",
      "Iteration 319, loss = 0.84898212\n",
      "Iteration 320, loss = 0.84956714\n",
      "Iteration 321, loss = 0.84840609\n",
      "Iteration 322, loss = 0.84551786\n",
      "Iteration 323, loss = 0.84742676\n",
      "Iteration 324, loss = 0.84666353\n",
      "Iteration 325, loss = 0.84816532\n",
      "Iteration 326, loss = 0.84661530\n",
      "Iteration 327, loss = 0.84702463\n",
      "Iteration 328, loss = 0.84533117\n",
      "Iteration 329, loss = 0.84638200\n",
      "Iteration 330, loss = 0.84496902\n",
      "Iteration 331, loss = 0.84671425\n",
      "Iteration 332, loss = 0.84407188\n",
      "Iteration 333, loss = 0.84426084\n",
      "Iteration 334, loss = 0.84036471\n",
      "Iteration 335, loss = 0.84304148\n",
      "Iteration 336, loss = 0.84421587\n",
      "Iteration 337, loss = 0.84327221\n",
      "Iteration 338, loss = 0.84110930\n",
      "Iteration 339, loss = 0.84020384\n",
      "Iteration 340, loss = 0.84062471\n",
      "Iteration 341, loss = 0.84309893\n",
      "Iteration 342, loss = 0.84038960\n",
      "Iteration 343, loss = 0.83881092\n",
      "Iteration 344, loss = 0.84021681\n",
      "Iteration 345, loss = 0.84067587\n",
      "Iteration 346, loss = 0.83905709\n",
      "Iteration 347, loss = 0.84305932\n",
      "Iteration 348, loss = 0.83911653\n",
      "Iteration 349, loss = 0.84170900\n",
      "Iteration 350, loss = 0.83971539\n",
      "Iteration 351, loss = 0.84151397\n",
      "Iteration 352, loss = 0.83765137\n",
      "Iteration 353, loss = 0.84007901\n",
      "Iteration 354, loss = 0.83850754\n",
      "Iteration 355, loss = 0.83842816\n",
      "Iteration 356, loss = 0.83978371\n",
      "Iteration 357, loss = 0.83950852\n",
      "Iteration 358, loss = 0.83679537\n",
      "Iteration 359, loss = 0.83579742\n",
      "Iteration 360, loss = 0.83487243\n",
      "Iteration 361, loss = 0.83589185\n",
      "Iteration 362, loss = 0.83603980\n",
      "Iteration 363, loss = 0.83327621\n",
      "Iteration 364, loss = 0.83431666\n",
      "Iteration 365, loss = 0.83541479\n",
      "Iteration 366, loss = 0.83708613\n",
      "Iteration 367, loss = 0.83514875\n",
      "Iteration 368, loss = 0.83251228\n",
      "Iteration 369, loss = 0.83168817\n",
      "Iteration 370, loss = 0.83325287\n",
      "Iteration 371, loss = 0.83054865\n",
      "Iteration 372, loss = 0.83367844\n",
      "Iteration 373, loss = 0.83388718\n",
      "Iteration 374, loss = 0.83370001\n",
      "Iteration 375, loss = 0.83036495\n",
      "Iteration 376, loss = 0.82861499\n",
      "Iteration 377, loss = 0.83239530\n",
      "Iteration 378, loss = 0.82893756\n",
      "Iteration 379, loss = 0.83150071\n",
      "Iteration 380, loss = 0.83162330\n",
      "Iteration 381, loss = 0.83001708\n",
      "Iteration 382, loss = 0.82916831\n",
      "Iteration 383, loss = 0.82842316\n",
      "Iteration 384, loss = 0.82692766\n",
      "Iteration 385, loss = 0.82888648\n",
      "Iteration 386, loss = 0.82881780\n",
      "Iteration 387, loss = 0.82691270\n",
      "Iteration 388, loss = 0.82839744\n",
      "Iteration 389, loss = 0.82404790\n",
      "Iteration 390, loss = 0.82928482\n",
      "Iteration 391, loss = 0.82405497\n",
      "Iteration 392, loss = 0.82903594\n",
      "Iteration 393, loss = 0.82534238\n",
      "Iteration 394, loss = 0.82794695\n",
      "Iteration 395, loss = 0.82762594\n",
      "Iteration 396, loss = 0.82536191\n",
      "Iteration 397, loss = 0.82518464\n",
      "Iteration 398, loss = 0.82590629\n",
      "Iteration 399, loss = 0.82342339\n",
      "Iteration 400, loss = 0.82579783\n",
      "Iteration 401, loss = 0.82360432\n",
      "Iteration 402, loss = 0.82632720\n",
      "Iteration 403, loss = 0.82481454\n",
      "Iteration 404, loss = 0.82635680\n",
      "Iteration 405, loss = 0.82308347\n",
      "Iteration 406, loss = 0.82070897\n",
      "Iteration 407, loss = 0.82404857\n",
      "Iteration 408, loss = 0.82310622\n",
      "Iteration 409, loss = 0.82253609\n",
      "Iteration 410, loss = 0.82187656\n",
      "Iteration 411, loss = 0.82291748\n",
      "Iteration 412, loss = 0.82082416\n",
      "Iteration 413, loss = 0.82184539\n",
      "Iteration 414, loss = 0.82162441\n",
      "Iteration 415, loss = 0.81907814\n",
      "Iteration 416, loss = 0.82070370\n",
      "Iteration 417, loss = 0.82178426\n",
      "Iteration 418, loss = 0.81744118\n",
      "Iteration 419, loss = 0.82076150\n",
      "Iteration 420, loss = 0.81933899\n",
      "Iteration 421, loss = 0.81925766\n",
      "Iteration 422, loss = 0.82093154\n",
      "Iteration 423, loss = 0.82190898\n",
      "Iteration 424, loss = 0.82017182\n",
      "Iteration 425, loss = 0.81881574\n",
      "Iteration 426, loss = 0.82194593\n",
      "Iteration 427, loss = 0.81754064\n",
      "Iteration 428, loss = 0.82003086\n",
      "Iteration 429, loss = 0.81860999\n",
      "Training loss did not improve more than tol=0.000000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.94925978\n",
      "Iteration 2, loss = 1.89200876\n",
      "Iteration 3, loss = 1.86800631\n",
      "Iteration 4, loss = 1.85272437\n",
      "Iteration 5, loss = 1.84066324\n",
      "Iteration 6, loss = 1.82948018\n",
      "Iteration 7, loss = 1.81790751\n",
      "Iteration 8, loss = 1.80530635\n",
      "Iteration 9, loss = 1.79163102\n",
      "Iteration 10, loss = 1.77720342\n",
      "Iteration 11, loss = 1.76142692\n",
      "Iteration 12, loss = 1.74548371\n",
      "Iteration 13, loss = 1.72911077\n",
      "Iteration 14, loss = 1.71237650\n",
      "Iteration 15, loss = 1.69580136\n",
      "Iteration 16, loss = 1.67909418\n",
      "Iteration 17, loss = 1.66194943\n",
      "Iteration 18, loss = 1.64523475\n",
      "Iteration 19, loss = 1.62783494\n",
      "Iteration 20, loss = 1.60975317\n",
      "Iteration 21, loss = 1.59072673\n",
      "Iteration 22, loss = 1.57188508\n",
      "Iteration 23, loss = 1.55301431\n",
      "Iteration 24, loss = 1.53510107\n",
      "Iteration 25, loss = 1.51636593\n",
      "Iteration 26, loss = 1.49860791\n",
      "Iteration 27, loss = 1.48168163\n",
      "Iteration 28, loss = 1.46723445\n",
      "Iteration 29, loss = 1.45226268\n",
      "Iteration 30, loss = 1.44119567\n",
      "Iteration 31, loss = 1.42921022\n",
      "Iteration 32, loss = 1.41951836\n",
      "Iteration 33, loss = 1.41163549\n",
      "Iteration 34, loss = 1.40337508\n",
      "Iteration 35, loss = 1.39650817\n",
      "Iteration 36, loss = 1.39004563\n",
      "Iteration 37, loss = 1.38372773\n",
      "Iteration 38, loss = 1.37798169\n",
      "Iteration 39, loss = 1.37321169\n",
      "Iteration 40, loss = 1.36836059\n",
      "Iteration 41, loss = 1.36481622\n",
      "Iteration 42, loss = 1.36185946\n",
      "Iteration 43, loss = 1.35611050\n",
      "Iteration 44, loss = 1.35262399\n",
      "Iteration 45, loss = 1.34830724\n",
      "Iteration 46, loss = 1.34528700\n",
      "Iteration 47, loss = 1.34087027\n",
      "Iteration 48, loss = 1.33938871\n",
      "Iteration 49, loss = 1.33465361\n",
      "Iteration 50, loss = 1.33098474\n",
      "Iteration 51, loss = 1.32751975\n",
      "Iteration 52, loss = 1.32382810\n",
      "Iteration 53, loss = 1.31976802\n",
      "Iteration 54, loss = 1.31530527\n",
      "Iteration 55, loss = 1.31167866\n",
      "Iteration 56, loss = 1.30822446\n",
      "Iteration 57, loss = 1.30488747\n",
      "Iteration 58, loss = 1.30081032\n",
      "Iteration 59, loss = 1.29710934\n",
      "Iteration 60, loss = 1.29266451\n",
      "Iteration 61, loss = 1.28840409\n",
      "Iteration 62, loss = 1.28512301\n",
      "Iteration 63, loss = 1.28093853\n",
      "Iteration 64, loss = 1.27647739\n",
      "Iteration 65, loss = 1.27242421\n",
      "Iteration 66, loss = 1.26752708\n",
      "Iteration 67, loss = 1.26250930\n",
      "Iteration 68, loss = 1.25865410\n",
      "Iteration 69, loss = 1.25554923\n",
      "Iteration 70, loss = 1.24895064\n",
      "Iteration 71, loss = 1.24488468\n",
      "Iteration 72, loss = 1.23938991\n",
      "Iteration 73, loss = 1.23547037\n",
      "Iteration 74, loss = 1.22999952\n",
      "Iteration 75, loss = 1.22615013\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 76, loss = 1.21971634\n",
      "Iteration 77, loss = 1.21523378\n",
      "Iteration 78, loss = 1.20952471\n",
      "Iteration 79, loss = 1.20398409\n",
      "Iteration 80, loss = 1.19877225\n",
      "Iteration 81, loss = 1.19671849\n",
      "Iteration 82, loss = 1.18980112\n",
      "Iteration 83, loss = 1.18416037\n",
      "Iteration 84, loss = 1.17859862\n",
      "Iteration 85, loss = 1.17428584\n",
      "Iteration 86, loss = 1.16865290\n",
      "Iteration 87, loss = 1.16484689\n",
      "Iteration 88, loss = 1.15924627\n",
      "Iteration 89, loss = 1.15419184\n",
      "Iteration 90, loss = 1.14935379\n",
      "Iteration 91, loss = 1.14427958\n",
      "Iteration 92, loss = 1.14000854\n",
      "Iteration 93, loss = 1.13575923\n",
      "Iteration 94, loss = 1.13003832\n",
      "Iteration 95, loss = 1.12569446\n",
      "Iteration 96, loss = 1.12245697\n",
      "Iteration 97, loss = 1.11716297\n",
      "Iteration 98, loss = 1.11295459\n",
      "Iteration 99, loss = 1.10862664\n",
      "Iteration 100, loss = 1.10489996\n",
      "Iteration 101, loss = 1.10063275\n",
      "Iteration 102, loss = 1.09709178\n",
      "Iteration 103, loss = 1.09428664\n",
      "Iteration 104, loss = 1.08844445\n",
      "Iteration 105, loss = 1.08735839\n",
      "Iteration 106, loss = 1.08269172\n",
      "Iteration 107, loss = 1.07758277\n",
      "Iteration 108, loss = 1.07521259\n",
      "Iteration 109, loss = 1.07175324\n",
      "Iteration 110, loss = 1.06874434\n",
      "Iteration 111, loss = 1.06504370\n",
      "Iteration 112, loss = 1.06228219\n",
      "Iteration 113, loss = 1.06058562\n",
      "Iteration 114, loss = 1.05771554\n",
      "Iteration 115, loss = 1.05397595\n",
      "Iteration 116, loss = 1.05056808\n",
      "Iteration 117, loss = 1.04699031\n",
      "Iteration 118, loss = 1.04537928\n",
      "Iteration 119, loss = 1.04176460\n",
      "Iteration 120, loss = 1.04045936\n",
      "Iteration 121, loss = 1.03795780\n",
      "Iteration 122, loss = 1.03506090\n",
      "Iteration 123, loss = 1.03293063\n",
      "Iteration 124, loss = 1.02905443\n",
      "Iteration 125, loss = 1.02746671\n",
      "Iteration 126, loss = 1.02657728\n",
      "Iteration 127, loss = 1.02341422\n",
      "Iteration 128, loss = 1.02109087\n",
      "Iteration 129, loss = 1.01852452\n",
      "Iteration 130, loss = 1.01632102\n",
      "Iteration 131, loss = 1.01314800\n",
      "Iteration 132, loss = 1.01378394\n",
      "Iteration 133, loss = 1.01191094\n",
      "Iteration 134, loss = 1.00923054\n",
      "Iteration 135, loss = 1.00606480\n",
      "Iteration 136, loss = 1.00498020\n",
      "Iteration 137, loss = 1.00367117\n",
      "Iteration 138, loss = 1.00025889\n",
      "Iteration 139, loss = 0.99932602\n",
      "Iteration 140, loss = 0.99598681\n",
      "Iteration 141, loss = 0.99643155\n",
      "Iteration 142, loss = 0.99427674\n",
      "Iteration 143, loss = 0.99233922\n",
      "Iteration 144, loss = 0.99076016\n",
      "Iteration 145, loss = 0.99101618\n",
      "Iteration 146, loss = 0.98911404\n",
      "Iteration 147, loss = 0.98773177\n",
      "Iteration 148, loss = 0.98455836\n",
      "Iteration 149, loss = 0.98390161\n",
      "Iteration 150, loss = 0.98123277\n",
      "Iteration 151, loss = 0.98241704\n",
      "Iteration 152, loss = 0.97857830\n",
      "Iteration 153, loss = 0.97692442\n",
      "Iteration 154, loss = 0.97544973\n",
      "Iteration 155, loss = 0.97525039\n",
      "Iteration 156, loss = 0.97419736\n",
      "Iteration 157, loss = 0.97156426\n",
      "Iteration 158, loss = 0.97153545\n",
      "Iteration 159, loss = 0.96966191\n",
      "Iteration 160, loss = 0.96957338\n",
      "Iteration 161, loss = 0.96844521\n",
      "Iteration 162, loss = 0.96761684\n",
      "Iteration 163, loss = 0.96359804\n",
      "Iteration 164, loss = 0.96415803\n",
      "Iteration 165, loss = 0.95995425\n",
      "Iteration 166, loss = 0.96109503\n",
      "Iteration 167, loss = 0.95962801\n",
      "Iteration 168, loss = 0.95904348\n",
      "Iteration 169, loss = 0.95671809\n",
      "Iteration 170, loss = 0.95724323\n",
      "Iteration 171, loss = 0.95533921\n",
      "Iteration 172, loss = 0.95418301\n",
      "Iteration 173, loss = 0.95141385\n",
      "Iteration 174, loss = 0.95224330\n",
      "Iteration 175, loss = 0.95101737\n",
      "Iteration 176, loss = 0.94819467\n",
      "Iteration 177, loss = 0.94865137\n",
      "Iteration 178, loss = 0.94786424\n",
      "Iteration 179, loss = 0.94675800\n",
      "Iteration 180, loss = 0.94533723\n",
      "Iteration 181, loss = 0.94437449\n",
      "Iteration 182, loss = 0.94430315\n",
      "Iteration 183, loss = 0.94195087\n",
      "Iteration 184, loss = 0.94306524\n",
      "Iteration 185, loss = 0.94019209\n",
      "Iteration 186, loss = 0.93824844\n",
      "Iteration 187, loss = 0.94095538\n",
      "Iteration 188, loss = 0.93718010\n",
      "Iteration 189, loss = 0.93932730\n",
      "Iteration 190, loss = 0.93742912\n",
      "Iteration 191, loss = 0.93429225\n",
      "Iteration 192, loss = 0.93305435\n",
      "Iteration 193, loss = 0.93363075\n",
      "Iteration 194, loss = 0.93154849\n",
      "Iteration 195, loss = 0.93181727\n",
      "Iteration 196, loss = 0.93222476\n",
      "Iteration 197, loss = 0.92958509\n",
      "Iteration 198, loss = 0.93073500\n",
      "Iteration 199, loss = 0.92911684\n",
      "Iteration 200, loss = 0.92700388\n",
      "Iteration 201, loss = 0.92749821\n",
      "Iteration 202, loss = 0.92547027\n",
      "Iteration 203, loss = 0.92504008\n",
      "Iteration 204, loss = 0.92467142\n",
      "Iteration 205, loss = 0.92330848\n",
      "Iteration 206, loss = 0.92466148\n",
      "Iteration 207, loss = 0.92096950\n",
      "Iteration 208, loss = 0.92180742\n",
      "Iteration 209, loss = 0.91965219\n",
      "Iteration 210, loss = 0.92044200\n",
      "Iteration 211, loss = 0.91790659\n",
      "Iteration 212, loss = 0.91708392\n",
      "Iteration 213, loss = 0.91704361\n",
      "Iteration 214, loss = 0.91758770\n",
      "Iteration 215, loss = 0.91728687\n",
      "Iteration 216, loss = 0.91469252\n",
      "Iteration 217, loss = 0.91579279\n",
      "Iteration 218, loss = 0.91616845\n",
      "Iteration 219, loss = 0.91525469\n",
      "Iteration 220, loss = 0.91136464\n",
      "Iteration 221, loss = 0.91251882\n",
      "Iteration 222, loss = 0.91113308\n",
      "Iteration 223, loss = 0.91386711\n",
      "Iteration 224, loss = 0.91046962\n",
      "Iteration 225, loss = 0.90969897\n",
      "Iteration 226, loss = 0.90893091\n",
      "Iteration 227, loss = 0.90967709\n",
      "Iteration 228, loss = 0.90978648\n",
      "Iteration 229, loss = 0.90939505\n",
      "Iteration 230, loss = 0.90530876\n",
      "Iteration 231, loss = 0.90602023\n",
      "Iteration 232, loss = 0.90684462\n",
      "Iteration 233, loss = 0.90394231\n",
      "Iteration 234, loss = 0.90245188\n",
      "Iteration 235, loss = 0.90267885\n",
      "Iteration 236, loss = 0.90107089\n",
      "Iteration 237, loss = 0.90464861\n",
      "Iteration 238, loss = 0.90175411\n",
      "Iteration 239, loss = 0.90194929\n",
      "Iteration 240, loss = 0.90181212\n",
      "Iteration 241, loss = 0.89935643\n",
      "Iteration 242, loss = 0.90007032\n",
      "Iteration 243, loss = 0.89780235\n",
      "Iteration 244, loss = 0.89947872\n",
      "Iteration 245, loss = 0.89908229\n",
      "Iteration 246, loss = 0.89641465\n",
      "Iteration 247, loss = 0.89678791\n",
      "Iteration 248, loss = 0.89626435\n",
      "Iteration 249, loss = 0.89449551\n",
      "Iteration 250, loss = 0.89614504\n",
      "Iteration 251, loss = 0.89591317\n",
      "Iteration 252, loss = 0.89439541\n",
      "Iteration 253, loss = 0.89389319\n",
      "Iteration 254, loss = 0.89077947\n",
      "Iteration 255, loss = 0.89155078\n",
      "Iteration 256, loss = 0.89132471\n",
      "Iteration 257, loss = 0.89170339\n",
      "Iteration 258, loss = 0.89285133\n",
      "Iteration 259, loss = 0.89017868\n",
      "Iteration 260, loss = 0.88851581\n",
      "Iteration 261, loss = 0.88936953\n",
      "Iteration 262, loss = 0.88806975\n",
      "Iteration 263, loss = 0.88672307\n",
      "Iteration 264, loss = 0.88998655\n",
      "Iteration 265, loss = 0.88590963\n",
      "Iteration 266, loss = 0.88629253\n",
      "Iteration 267, loss = 0.88428738\n",
      "Iteration 268, loss = 0.88758610\n",
      "Iteration 269, loss = 0.88469451\n",
      "Iteration 270, loss = 0.88487527\n",
      "Iteration 271, loss = 0.88351528\n",
      "Iteration 272, loss = 0.88400590\n",
      "Iteration 273, loss = 0.88353612\n",
      "Iteration 274, loss = 0.88280512\n",
      "Iteration 275, loss = 0.88171350\n",
      "Iteration 276, loss = 0.88273277\n",
      "Iteration 277, loss = 0.88335329\n",
      "Iteration 278, loss = 0.88078215\n",
      "Iteration 279, loss = 0.88092218\n",
      "Iteration 280, loss = 0.87999262\n",
      "Iteration 281, loss = 0.87943629\n",
      "Iteration 282, loss = 0.87903143\n",
      "Iteration 283, loss = 0.87744292\n",
      "Iteration 284, loss = 0.87822225\n",
      "Iteration 285, loss = 0.87584858\n",
      "Iteration 286, loss = 0.87725273\n",
      "Iteration 287, loss = 0.87698128\n",
      "Iteration 288, loss = 0.87692623\n",
      "Iteration 289, loss = 0.87815279\n",
      "Iteration 290, loss = 0.87409042\n",
      "Iteration 291, loss = 0.87461422\n",
      "Iteration 292, loss = 0.87653124\n",
      "Iteration 293, loss = 0.87629533\n",
      "Iteration 294, loss = 0.87364490\n",
      "Iteration 295, loss = 0.87419931\n",
      "Iteration 296, loss = 0.87335616\n",
      "Iteration 297, loss = 0.87299923\n",
      "Iteration 298, loss = 0.87194972\n",
      "Iteration 299, loss = 0.87112541\n",
      "Iteration 300, loss = 0.87267729\n",
      "Iteration 301, loss = 0.87326262\n",
      "Iteration 302, loss = 0.87384662\n",
      "Iteration 303, loss = 0.87244619\n",
      "Iteration 304, loss = 0.86880205\n",
      "Iteration 305, loss = 0.87105983\n",
      "Iteration 306, loss = 0.86915486\n",
      "Iteration 307, loss = 0.86883894\n",
      "Iteration 308, loss = 0.86872324\n",
      "Iteration 309, loss = 0.86746605\n",
      "Iteration 310, loss = 0.86578782\n",
      "Iteration 311, loss = 0.86802339\n",
      "Iteration 312, loss = 0.86654515\n",
      "Iteration 313, loss = 0.86782378\n",
      "Iteration 314, loss = 0.86643814\n",
      "Iteration 315, loss = 0.86935188\n",
      "Iteration 316, loss = 0.86696429\n",
      "Iteration 317, loss = 0.86653926\n",
      "Iteration 318, loss = 0.86725725\n",
      "Iteration 319, loss = 0.86488224\n",
      "Iteration 320, loss = 0.86428125\n",
      "Iteration 321, loss = 0.86702010\n",
      "Iteration 322, loss = 0.86247445\n",
      "Iteration 323, loss = 0.86461207\n",
      "Iteration 324, loss = 0.86195276\n",
      "Iteration 325, loss = 0.86388358\n",
      "Iteration 326, loss = 0.86456924\n",
      "Iteration 327, loss = 0.86113164\n",
      "Iteration 328, loss = 0.86198481\n",
      "Iteration 329, loss = 0.86327942\n",
      "Iteration 330, loss = 0.86047024\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 331, loss = 0.85869382\n",
      "Iteration 332, loss = 0.85847511\n",
      "Iteration 333, loss = 0.86028959\n",
      "Iteration 334, loss = 0.86074387\n",
      "Iteration 335, loss = 0.85987204\n",
      "Iteration 336, loss = 0.85830332\n",
      "Iteration 337, loss = 0.85738998\n",
      "Iteration 338, loss = 0.85699043\n",
      "Iteration 339, loss = 0.85754420\n",
      "Iteration 340, loss = 0.85889078\n",
      "Iteration 341, loss = 0.85823005\n",
      "Iteration 342, loss = 0.85520754\n",
      "Iteration 343, loss = 0.85782951\n",
      "Iteration 344, loss = 0.85660000\n",
      "Iteration 345, loss = 0.85391658\n",
      "Iteration 346, loss = 0.85253747\n",
      "Iteration 347, loss = 0.85566488\n",
      "Iteration 348, loss = 0.85499739\n",
      "Iteration 349, loss = 0.85285928\n",
      "Iteration 350, loss = 0.85199536\n",
      "Iteration 351, loss = 0.85312139\n",
      "Iteration 352, loss = 0.85365295\n",
      "Iteration 353, loss = 0.85043505\n",
      "Iteration 354, loss = 0.85042199\n",
      "Iteration 355, loss = 0.85129732\n",
      "Iteration 356, loss = 0.85247599\n",
      "Iteration 357, loss = 0.85283604\n",
      "Iteration 358, loss = 0.85085491\n",
      "Iteration 359, loss = 0.85040658\n",
      "Iteration 360, loss = 0.84721410\n",
      "Iteration 361, loss = 0.84686687\n",
      "Iteration 362, loss = 0.84925924\n",
      "Iteration 363, loss = 0.85006089\n",
      "Iteration 364, loss = 0.84639717\n",
      "Iteration 365, loss = 0.84822786\n",
      "Iteration 366, loss = 0.84566202\n",
      "Iteration 367, loss = 0.84771064\n",
      "Iteration 368, loss = 0.84604817\n",
      "Iteration 369, loss = 0.84608855\n",
      "Iteration 370, loss = 0.84640205\n",
      "Iteration 371, loss = 0.84594013\n",
      "Iteration 372, loss = 0.84675399\n",
      "Iteration 373, loss = 0.84404905\n",
      "Iteration 374, loss = 0.84593921\n",
      "Iteration 375, loss = 0.84311823\n",
      "Iteration 376, loss = 0.84493451\n",
      "Iteration 377, loss = 0.84294761\n",
      "Iteration 378, loss = 0.84223877\n",
      "Iteration 379, loss = 0.84346592\n",
      "Iteration 380, loss = 0.84165594\n",
      "Iteration 381, loss = 0.84355382\n",
      "Iteration 382, loss = 0.84137110\n",
      "Iteration 383, loss = 0.84182685\n",
      "Iteration 384, loss = 0.84004740\n",
      "Iteration 385, loss = 0.84107688\n",
      "Iteration 386, loss = 0.83643645\n",
      "Iteration 387, loss = 0.83929752\n",
      "Iteration 388, loss = 0.83930823\n",
      "Iteration 389, loss = 0.84166930\n",
      "Iteration 390, loss = 0.83730322\n",
      "Iteration 391, loss = 0.83944180\n",
      "Iteration 392, loss = 0.83763088\n",
      "Iteration 393, loss = 0.83758136\n",
      "Iteration 394, loss = 0.83670023\n",
      "Iteration 395, loss = 0.83683183\n",
      "Iteration 396, loss = 0.83789609\n",
      "Iteration 397, loss = 0.83659848\n",
      "Training loss did not improve more than tol=0.000000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.10182876\n",
      "Iteration 2, loss = 1.94733623\n",
      "Iteration 3, loss = 1.86983589\n",
      "Iteration 4, loss = 1.83371283\n",
      "Iteration 5, loss = 1.81267768\n",
      "Iteration 6, loss = 1.79744809\n",
      "Iteration 7, loss = 1.78430570\n",
      "Iteration 8, loss = 1.77294264\n",
      "Iteration 9, loss = 1.76212589\n",
      "Iteration 10, loss = 1.75221193\n",
      "Iteration 11, loss = 1.74248633\n",
      "Iteration 12, loss = 1.73292511\n",
      "Iteration 13, loss = 1.72326981\n",
      "Iteration 14, loss = 1.71308416\n",
      "Iteration 15, loss = 1.70086599\n",
      "Iteration 16, loss = 1.68838449\n",
      "Iteration 17, loss = 1.67476795\n",
      "Iteration 18, loss = 1.65964934\n",
      "Iteration 19, loss = 1.64346506\n",
      "Iteration 20, loss = 1.62597330\n",
      "Iteration 21, loss = 1.60741413\n",
      "Iteration 22, loss = 1.58832343\n",
      "Iteration 23, loss = 1.56749481\n",
      "Iteration 24, loss = 1.54666117\n",
      "Iteration 25, loss = 1.52503189\n",
      "Iteration 26, loss = 1.50305327\n",
      "Iteration 27, loss = 1.48110915\n",
      "Iteration 28, loss = 1.45974737\n",
      "Iteration 29, loss = 1.44018515\n",
      "Iteration 30, loss = 1.42116241\n",
      "Iteration 31, loss = 1.40437764\n",
      "Iteration 32, loss = 1.38888681\n",
      "Iteration 33, loss = 1.37561994\n",
      "Iteration 34, loss = 1.36369830\n",
      "Iteration 35, loss = 1.35356094\n",
      "Iteration 36, loss = 1.34456153\n",
      "Iteration 37, loss = 1.33588674\n",
      "Iteration 38, loss = 1.32872850\n",
      "Iteration 39, loss = 1.32201303\n",
      "Iteration 40, loss = 1.31691701\n",
      "Iteration 41, loss = 1.31085291\n",
      "Iteration 42, loss = 1.30675407\n",
      "Iteration 43, loss = 1.30265310\n",
      "Iteration 44, loss = 1.29774468\n",
      "Iteration 45, loss = 1.29327575\n",
      "Iteration 46, loss = 1.28966487\n",
      "Iteration 47, loss = 1.28579288\n",
      "Iteration 48, loss = 1.28245185\n",
      "Iteration 49, loss = 1.27903313\n",
      "Iteration 50, loss = 1.27458383\n",
      "Iteration 51, loss = 1.27081451\n",
      "Iteration 52, loss = 1.26738227\n",
      "Iteration 53, loss = 1.26459545\n",
      "Iteration 54, loss = 1.26101537\n",
      "Iteration 55, loss = 1.25699518\n",
      "Iteration 56, loss = 1.25436987\n",
      "Iteration 57, loss = 1.25194392\n",
      "Iteration 58, loss = 1.24531612\n",
      "Iteration 59, loss = 1.24236206\n",
      "Iteration 60, loss = 1.23997100\n",
      "Iteration 61, loss = 1.23511595\n",
      "Iteration 62, loss = 1.23239037\n",
      "Iteration 63, loss = 1.22799208\n",
      "Iteration 64, loss = 1.22362308\n",
      "Iteration 65, loss = 1.22062447\n",
      "Iteration 66, loss = 1.21610653\n",
      "Iteration 67, loss = 1.21195276\n",
      "Iteration 68, loss = 1.20755161\n",
      "Iteration 69, loss = 1.20406648\n",
      "Iteration 70, loss = 1.20069220\n",
      "Iteration 71, loss = 1.19615941\n",
      "Iteration 72, loss = 1.19100154\n",
      "Iteration 73, loss = 1.18726355\n",
      "Iteration 74, loss = 1.18223573\n",
      "Iteration 75, loss = 1.17960517\n",
      "Iteration 76, loss = 1.17473728\n",
      "Iteration 77, loss = 1.16946237\n",
      "Iteration 78, loss = 1.16534069\n",
      "Iteration 79, loss = 1.15988315\n",
      "Iteration 80, loss = 1.15571343\n",
      "Iteration 81, loss = 1.15079328\n",
      "Iteration 82, loss = 1.14607244\n",
      "Iteration 83, loss = 1.14300642\n",
      "Iteration 84, loss = 1.13764419\n",
      "Iteration 85, loss = 1.13260988\n",
      "Iteration 86, loss = 1.12949063\n",
      "Iteration 87, loss = 1.12457262\n",
      "Iteration 88, loss = 1.12115682\n",
      "Iteration 89, loss = 1.11672317\n",
      "Iteration 90, loss = 1.11118992\n",
      "Iteration 91, loss = 1.10857419\n",
      "Iteration 92, loss = 1.10357627\n",
      "Iteration 93, loss = 1.10004439\n",
      "Iteration 94, loss = 1.09542823\n",
      "Iteration 95, loss = 1.09178759\n",
      "Iteration 96, loss = 1.08811712\n",
      "Iteration 97, loss = 1.08478549\n",
      "Iteration 98, loss = 1.07995376\n",
      "Iteration 99, loss = 1.07781713\n",
      "Iteration 100, loss = 1.07336582\n",
      "Iteration 101, loss = 1.07014086\n",
      "Iteration 102, loss = 1.06736844\n",
      "Iteration 103, loss = 1.06377793\n",
      "Iteration 104, loss = 1.05947572\n",
      "Iteration 105, loss = 1.05746185\n",
      "Iteration 106, loss = 1.05443908\n",
      "Iteration 107, loss = 1.05159267\n",
      "Iteration 108, loss = 1.04764561\n",
      "Iteration 109, loss = 1.04632090\n",
      "Iteration 110, loss = 1.04459971\n",
      "Iteration 111, loss = 1.03878207\n",
      "Iteration 112, loss = 1.03775231\n",
      "Iteration 113, loss = 1.03435121\n",
      "Iteration 114, loss = 1.03301617\n",
      "Iteration 115, loss = 1.02947793\n",
      "Iteration 116, loss = 1.02581549\n",
      "Iteration 117, loss = 1.02359191\n",
      "Iteration 118, loss = 1.02016741\n",
      "Iteration 119, loss = 1.01862817\n",
      "Iteration 120, loss = 1.01517687\n",
      "Iteration 121, loss = 1.01277208\n",
      "Iteration 122, loss = 1.00978682\n",
      "Iteration 123, loss = 1.00812990\n",
      "Iteration 124, loss = 1.00623031\n",
      "Iteration 125, loss = 1.00296348\n",
      "Iteration 126, loss = 1.00044272\n",
      "Iteration 127, loss = 0.99729966\n",
      "Iteration 128, loss = 0.99592925\n",
      "Iteration 129, loss = 0.99387012\n",
      "Iteration 130, loss = 0.99191562\n",
      "Iteration 131, loss = 0.99003897\n",
      "Iteration 132, loss = 0.98834311\n",
      "Iteration 133, loss = 0.98562784\n",
      "Iteration 134, loss = 0.98433190\n",
      "Iteration 135, loss = 0.98070324\n",
      "Iteration 136, loss = 0.97853288\n",
      "Iteration 137, loss = 0.97827750\n",
      "Iteration 138, loss = 0.97749961\n",
      "Iteration 139, loss = 0.97401553\n",
      "Iteration 140, loss = 0.97319106\n",
      "Iteration 141, loss = 0.97172473\n",
      "Iteration 142, loss = 0.96809407\n",
      "Iteration 143, loss = 0.96626382\n",
      "Iteration 144, loss = 0.96514448\n",
      "Iteration 145, loss = 0.96315504\n",
      "Iteration 146, loss = 0.96215813\n",
      "Iteration 147, loss = 0.96059847\n",
      "Iteration 148, loss = 0.95893099\n",
      "Iteration 149, loss = 0.95744130\n",
      "Iteration 150, loss = 0.95659346\n",
      "Iteration 151, loss = 0.95298964\n",
      "Iteration 152, loss = 0.95213596\n",
      "Iteration 153, loss = 0.95105399\n",
      "Iteration 154, loss = 0.94867962\n",
      "Iteration 155, loss = 0.94741209\n",
      "Iteration 156, loss = 0.94752302\n",
      "Iteration 157, loss = 0.94413275\n",
      "Iteration 158, loss = 0.94263108\n",
      "Iteration 159, loss = 0.93981805\n",
      "Iteration 160, loss = 0.93922543\n",
      "Iteration 161, loss = 0.93766324\n",
      "Iteration 162, loss = 0.93821775\n",
      "Iteration 163, loss = 0.93517388\n",
      "Iteration 164, loss = 0.93469761\n",
      "Iteration 165, loss = 0.93085187\n",
      "Iteration 166, loss = 0.92999372\n",
      "Iteration 167, loss = 0.92768324\n",
      "Iteration 168, loss = 0.92813580\n",
      "Iteration 169, loss = 0.92858109\n",
      "Iteration 170, loss = 0.92385553\n",
      "Iteration 171, loss = 0.92343113\n",
      "Iteration 172, loss = 0.92117848\n",
      "Iteration 173, loss = 0.92116955\n",
      "Iteration 174, loss = 0.92050071\n",
      "Iteration 175, loss = 0.91807289\n",
      "Iteration 176, loss = 0.91682354\n",
      "Iteration 177, loss = 0.91454799\n",
      "Iteration 178, loss = 0.91513490\n",
      "Iteration 179, loss = 0.91424542\n",
      "Iteration 180, loss = 0.91068479\n",
      "Iteration 181, loss = 0.90890975\n",
      "Iteration 182, loss = 0.90966380\n",
      "Iteration 183, loss = 0.90679881\n",
      "Iteration 184, loss = 0.90624111\n",
      "Iteration 185, loss = 0.90406128\n",
      "Iteration 186, loss = 0.90338366\n",
      "Iteration 187, loss = 0.90218215\n",
      "Iteration 188, loss = 0.90091206\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 189, loss = 0.90110019\n",
      "Iteration 190, loss = 0.89763016\n",
      "Iteration 191, loss = 0.89847359\n",
      "Iteration 192, loss = 0.89667031\n",
      "Iteration 193, loss = 0.89639470\n",
      "Iteration 194, loss = 0.89471165\n",
      "Iteration 195, loss = 0.89296885\n",
      "Iteration 196, loss = 0.89569274\n",
      "Iteration 197, loss = 0.89052292\n",
      "Iteration 198, loss = 0.89066188\n",
      "Iteration 199, loss = 0.88927442\n",
      "Iteration 200, loss = 0.88832950\n",
      "Iteration 201, loss = 0.88683464\n",
      "Iteration 202, loss = 0.88740026\n",
      "Iteration 203, loss = 0.88537319\n",
      "Iteration 204, loss = 0.88407176\n",
      "Iteration 205, loss = 0.88194993\n",
      "Iteration 206, loss = 0.88171627\n",
      "Iteration 207, loss = 0.88176469\n",
      "Iteration 208, loss = 0.88071659\n",
      "Iteration 209, loss = 0.87876899\n",
      "Iteration 210, loss = 0.87671243\n",
      "Iteration 211, loss = 0.87670176\n",
      "Iteration 212, loss = 0.87720670\n",
      "Iteration 213, loss = 0.87473390\n",
      "Iteration 214, loss = 0.87393057\n",
      "Iteration 215, loss = 0.87426151\n",
      "Iteration 216, loss = 0.87199375\n",
      "Iteration 217, loss = 0.87187222\n",
      "Iteration 218, loss = 0.87311312\n",
      "Iteration 219, loss = 0.87134053\n",
      "Iteration 220, loss = 0.87082659\n",
      "Iteration 221, loss = 0.87232422\n",
      "Iteration 222, loss = 0.86836213\n",
      "Iteration 223, loss = 0.86974467\n",
      "Iteration 224, loss = 0.86626830\n",
      "Iteration 225, loss = 0.86898089\n",
      "Iteration 226, loss = 0.86590274\n",
      "Iteration 227, loss = 0.86535398\n",
      "Iteration 228, loss = 0.86408777\n",
      "Iteration 229, loss = 0.86413047\n",
      "Iteration 230, loss = 0.86106595\n",
      "Iteration 231, loss = 0.86288680\n",
      "Iteration 232, loss = 0.86143802\n",
      "Iteration 233, loss = 0.86123479\n",
      "Iteration 234, loss = 0.85757340\n",
      "Iteration 235, loss = 0.85968911\n",
      "Iteration 236, loss = 0.86014779\n",
      "Iteration 237, loss = 0.85887128\n",
      "Iteration 238, loss = 0.85849873\n",
      "Iteration 239, loss = 0.85694888\n",
      "Iteration 240, loss = 0.85866640\n",
      "Iteration 241, loss = 0.85518710\n",
      "Iteration 242, loss = 0.85612012\n",
      "Iteration 243, loss = 0.85270597\n",
      "Iteration 244, loss = 0.85391902\n",
      "Iteration 245, loss = 0.85303538\n",
      "Iteration 246, loss = 0.85149325\n",
      "Iteration 247, loss = 0.85313739\n",
      "Iteration 248, loss = 0.85180648\n",
      "Iteration 249, loss = 0.85062865\n",
      "Iteration 250, loss = 0.84880478\n",
      "Iteration 251, loss = 0.84875638\n",
      "Iteration 252, loss = 0.85123631\n",
      "Iteration 253, loss = 0.84992999\n",
      "Iteration 254, loss = 0.84814609\n",
      "Iteration 255, loss = 0.84668669\n",
      "Iteration 256, loss = 0.84670770\n",
      "Iteration 257, loss = 0.84835089\n",
      "Iteration 258, loss = 0.84410472\n",
      "Iteration 259, loss = 0.84465089\n",
      "Iteration 260, loss = 0.84605677\n",
      "Iteration 261, loss = 0.84315766\n",
      "Iteration 262, loss = 0.84297500\n",
      "Iteration 263, loss = 0.84145226\n",
      "Iteration 264, loss = 0.84279305\n",
      "Iteration 265, loss = 0.84446300\n",
      "Iteration 266, loss = 0.83999858\n",
      "Iteration 267, loss = 0.84106317\n",
      "Iteration 268, loss = 0.83862003\n",
      "Iteration 269, loss = 0.84007378\n",
      "Iteration 270, loss = 0.84024955\n",
      "Iteration 271, loss = 0.83907655\n",
      "Iteration 272, loss = 0.83831845\n",
      "Iteration 273, loss = 0.83932310\n",
      "Iteration 274, loss = 0.83808979\n",
      "Iteration 275, loss = 0.83743929\n",
      "Iteration 276, loss = 0.83587438\n",
      "Iteration 277, loss = 0.83443832\n",
      "Iteration 278, loss = 0.83357753\n",
      "Iteration 279, loss = 0.83431398\n",
      "Iteration 280, loss = 0.83467899\n",
      "Iteration 281, loss = 0.83431837\n",
      "Iteration 282, loss = 0.83266096\n",
      "Iteration 283, loss = 0.83173091\n",
      "Iteration 284, loss = 0.83317068\n",
      "Iteration 285, loss = 0.83161913\n",
      "Iteration 286, loss = 0.83126478\n",
      "Iteration 287, loss = 0.82970594\n",
      "Iteration 288, loss = 0.82911156\n",
      "Iteration 289, loss = 0.82853319\n",
      "Iteration 290, loss = 0.83082898\n",
      "Iteration 291, loss = 0.82774651\n",
      "Iteration 292, loss = 0.82768421\n",
      "Iteration 293, loss = 0.82812172\n",
      "Iteration 294, loss = 0.82804083\n",
      "Iteration 295, loss = 0.82976487\n",
      "Iteration 296, loss = 0.82547309\n",
      "Iteration 297, loss = 0.82614465\n",
      "Iteration 298, loss = 0.82641750\n",
      "Iteration 299, loss = 0.82713805\n",
      "Iteration 300, loss = 0.82683572\n",
      "Iteration 301, loss = 0.82376270\n",
      "Iteration 302, loss = 0.82398948\n",
      "Iteration 303, loss = 0.82228552\n",
      "Iteration 304, loss = 0.82278271\n",
      "Iteration 305, loss = 0.82470908\n",
      "Iteration 306, loss = 0.82501445\n",
      "Iteration 307, loss = 0.82291009\n",
      "Iteration 308, loss = 0.82121083\n",
      "Iteration 309, loss = 0.82067032\n",
      "Iteration 310, loss = 0.82246692\n",
      "Iteration 311, loss = 0.81828034\n",
      "Iteration 312, loss = 0.82024022\n",
      "Iteration 313, loss = 0.82013576\n",
      "Iteration 314, loss = 0.81966296\n",
      "Iteration 315, loss = 0.81840547\n",
      "Iteration 316, loss = 0.81890552\n",
      "Iteration 317, loss = 0.81734535\n",
      "Iteration 318, loss = 0.81725623\n",
      "Iteration 319, loss = 0.81588054\n",
      "Iteration 320, loss = 0.81819577\n",
      "Iteration 321, loss = 0.81573253\n",
      "Iteration 322, loss = 0.81389829\n",
      "Iteration 323, loss = 0.81545968\n",
      "Iteration 324, loss = 0.81504207\n",
      "Iteration 325, loss = 0.81230539\n",
      "Iteration 326, loss = 0.81324970\n",
      "Iteration 327, loss = 0.81419094\n",
      "Iteration 328, loss = 0.81348957\n",
      "Iteration 329, loss = 0.81147617\n",
      "Iteration 330, loss = 0.81011573\n",
      "Iteration 331, loss = 0.81112351\n",
      "Iteration 332, loss = 0.81032753\n",
      "Iteration 333, loss = 0.80938190\n",
      "Iteration 334, loss = 0.81196168\n",
      "Iteration 335, loss = 0.81207798\n",
      "Iteration 336, loss = 0.80915291\n",
      "Iteration 337, loss = 0.80943951\n",
      "Iteration 338, loss = 0.80882482\n",
      "Iteration 339, loss = 0.80954404\n",
      "Iteration 340, loss = 0.80716339\n",
      "Iteration 341, loss = 0.80862287\n",
      "Iteration 342, loss = 0.80742010\n",
      "Iteration 343, loss = 0.80740162\n",
      "Iteration 344, loss = 0.80616687\n",
      "Iteration 345, loss = 0.80726390\n",
      "Iteration 346, loss = 0.80502418\n",
      "Iteration 347, loss = 0.80499788\n",
      "Iteration 348, loss = 0.80669766\n",
      "Iteration 349, loss = 0.80379415\n",
      "Iteration 350, loss = 0.80459066\n",
      "Iteration 351, loss = 0.80555305\n",
      "Iteration 352, loss = 0.80447909\n",
      "Iteration 353, loss = 0.80352537\n",
      "Iteration 354, loss = 0.80419062\n",
      "Iteration 355, loss = 0.80139592\n",
      "Iteration 356, loss = 0.80134055\n",
      "Iteration 357, loss = 0.80090622\n",
      "Iteration 358, loss = 0.80148290\n",
      "Iteration 359, loss = 0.80173653\n",
      "Iteration 360, loss = 0.80322988\n",
      "Iteration 361, loss = 0.80070932\n",
      "Iteration 362, loss = 0.80117790\n",
      "Iteration 363, loss = 0.80166960\n",
      "Iteration 364, loss = 0.80030640\n",
      "Iteration 365, loss = 0.79916344\n",
      "Iteration 366, loss = 0.80151456\n",
      "Iteration 367, loss = 0.79679094\n",
      "Iteration 368, loss = 0.79773181\n",
      "Iteration 369, loss = 0.80101037\n",
      "Iteration 370, loss = 0.79971105\n",
      "Iteration 371, loss = 0.79764378\n",
      "Iteration 372, loss = 0.79800624\n",
      "Iteration 373, loss = 0.79673750\n",
      "Iteration 374, loss = 0.79689386\n",
      "Iteration 375, loss = 0.79656053\n",
      "Iteration 376, loss = 0.79768476\n",
      "Iteration 377, loss = 0.79837928\n",
      "Iteration 378, loss = 0.79556257\n",
      "Iteration 379, loss = 0.79588043\n",
      "Iteration 380, loss = 0.79454912\n",
      "Iteration 381, loss = 0.79140579\n",
      "Iteration 382, loss = 0.79495945\n",
      "Iteration 383, loss = 0.79382620\n",
      "Iteration 384, loss = 0.79578735\n",
      "Iteration 385, loss = 0.79246030\n",
      "Iteration 386, loss = 0.79439033\n",
      "Iteration 387, loss = 0.79399873\n",
      "Iteration 388, loss = 0.79220576\n",
      "Iteration 389, loss = 0.79369747\n",
      "Iteration 390, loss = 0.79296608\n",
      "Iteration 391, loss = 0.79165113\n",
      "Iteration 392, loss = 0.79114055\n",
      "Iteration 393, loss = 0.79325678\n",
      "Iteration 394, loss = 0.79417178\n",
      "Iteration 395, loss = 0.79221319\n",
      "Iteration 396, loss = 0.79188556\n",
      "Iteration 397, loss = 0.79017721\n",
      "Iteration 398, loss = 0.79329315\n",
      "Iteration 399, loss = 0.79129350\n",
      "Iteration 400, loss = 0.79331075\n",
      "Iteration 401, loss = 0.79073858\n",
      "Iteration 402, loss = 0.78936541\n",
      "Iteration 403, loss = 0.78911797\n",
      "Iteration 404, loss = 0.78950792\n",
      "Iteration 405, loss = 0.78849572\n",
      "Iteration 406, loss = 0.78939111\n",
      "Iteration 407, loss = 0.78759041\n",
      "Iteration 408, loss = 0.78884560\n",
      "Iteration 409, loss = 0.78923815\n",
      "Iteration 410, loss = 0.78729314\n",
      "Iteration 411, loss = 0.78746259\n",
      "Iteration 412, loss = 0.78845410\n",
      "Iteration 413, loss = 0.78698316\n",
      "Iteration 414, loss = 0.78805450\n",
      "Iteration 415, loss = 0.78450600\n",
      "Iteration 416, loss = 0.78518800\n",
      "Iteration 417, loss = 0.78749683\n",
      "Iteration 418, loss = 0.78674717\n",
      "Iteration 419, loss = 0.78621631\n",
      "Iteration 420, loss = 0.78600798\n",
      "Iteration 421, loss = 0.78505246\n",
      "Iteration 422, loss = 0.78544655\n",
      "Iteration 423, loss = 0.78478283\n",
      "Iteration 424, loss = 0.78442053\n",
      "Iteration 425, loss = 0.78592005\n",
      "Iteration 426, loss = 0.78418025\n",
      "Iteration 427, loss = 0.78410052\n",
      "Iteration 428, loss = 0.78296680\n",
      "Iteration 429, loss = 0.78578488\n",
      "Iteration 430, loss = 0.78275073\n",
      "Iteration 431, loss = 0.78262746\n",
      "Iteration 432, loss = 0.78220767\n",
      "Iteration 433, loss = 0.78273747\n",
      "Iteration 434, loss = 0.78292319\n",
      "Iteration 435, loss = 0.78226553\n",
      "Iteration 436, loss = 0.78199877\n",
      "Iteration 437, loss = 0.78105222\n",
      "Iteration 438, loss = 0.78188649\n",
      "Iteration 439, loss = 0.78403682\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 440, loss = 0.78268265\n",
      "Iteration 441, loss = 0.78096959\n",
      "Iteration 442, loss = 0.78260738\n",
      "Iteration 443, loss = 0.78024107\n",
      "Iteration 444, loss = 0.78102701\n",
      "Iteration 445, loss = 0.78351877\n",
      "Iteration 446, loss = 0.77913994\n",
      "Iteration 447, loss = 0.78052912\n",
      "Iteration 448, loss = 0.77533743\n",
      "Iteration 449, loss = 0.77877651\n",
      "Iteration 450, loss = 0.77790068\n",
      "Iteration 451, loss = 0.77922285\n",
      "Iteration 452, loss = 0.77645403\n",
      "Iteration 453, loss = 0.77893907\n",
      "Iteration 454, loss = 0.77866510\n",
      "Iteration 455, loss = 0.77812485\n",
      "Iteration 456, loss = 0.77767440\n",
      "Iteration 457, loss = 0.77772924\n",
      "Iteration 458, loss = 0.77778552\n",
      "Iteration 459, loss = 0.77867707\n",
      "Training loss did not improve more than tol=0.000000 for 10 consecutive epochs. Stopping.\n",
      "Time:  33.9093366\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "import timeit\n",
    "\n",
    "#uncomment to reproduce results\n",
    "#state1=('MT19937', [1546483054, 3151018956, 3632092744, 2091789978,  300308011, 3542745596, 1822245025, 1174413095, 2428608518, 3408020893, 186189152, 1784643307, 1123961950, 2949380232, 2269177984, 1284984153, 1836948424, 2037122910, 4280926093,  668832025, 3502219249, 1607507087, 2700591644,  259236845, 1926529433, 2992208145, 2459758489, 3311500098, 3576850881, 3553266599, 4112505810, 4041970324, 3565462707, 2504468881,  559419937, 3025784872, 2700843702, 1408916777, 2310613230,  459764035, 2285996695, 2065197554,  184145929, 3178106040, 1917882254, 2005215896,  562343819, 2887230726, 3216981700, 1047902799, 290441309, 1027527140, 3042431784,  164481479, 2006784697, 2672820943, 1001115673, 3117775638, 1570980638,  976097910, 3846729418,  207373962, 1743630000, 2786698516,  219211758, 4081835815, 3780463990,  122563692, 2123453152, 3839302666, 4145930451, 1640563287, 2129130038,   94568188, 3165091254, 872072271, 1730591351, 1296742107, 1917505104, 4028874564, 241727059, 3603831312, 3318314193, 2713549101, 1508112095, 3726586859, 2691605214, 3302096163, 1932459512, 2504225942, 2634822334,  807051399, 2451215519,  709863246, 3440885796, 1430564546, 1895059023, 1517069095, 1312142432, 1935610792, 1000813329, 2495758106, 2114895070, 3707558754, 1825601197, 1282727493, 2028606782,  577902918, 3748949770, 1301979130, 2709221749,  330134370,  366737434,  407162035, 2174062097, 260978930, 3862165998, 1958506486,  704928745, 3091657828, 1670578870, 1014301100,  331527510, 1010209129,  993497065, 1414428778, 2958280117,   31256754, 2782410938, 1370544409, 4003167994, 3743111616, 1630061939, 2766006399, 3831668439, 2847153707, 2001917877, 1057648269, 4129428267, 3184129875, 3846171777, 4149731799,  978488626,  612333385, 2887721309, 2394751500, 1829588504, 3552194928,  185667059, 1675005044, 1745029055,  109624461, 1689573689,  337685425, 3399251567, 2014217527, 2407429354, 1540114469, 3006249682,  410911919, 251822251, 3951380248, 2489286985, 3538691642, 2939065889, 3937335156,  834097561, 2133660420, 3977850529, 2740066451, 914377439, 1221058982, 1311729295, 3967251635, 2841852446, 1426254267, 4115710226, 1940211046, 1814659413, 3689020119, 4214188632, 3323767196,  929956721, 2186663244,  405137022, 1153392239, 2248876064,  260300837, 1862155349, 4007838177, 1046900968, 1731114055,  701535838, 3830817239, 3324988774, 987207324, 2230052944, 2728692511, 1126721591, 3224372245, 3921372022, 2472205042, 4271034746, 3361040008,  794744995, 584037404, 2685995994,   69223687, 3168598931, 3536247302, 2772677067,  374912544, 3310812020, 2082792904, 3456514595, 3027242871, 4037981953, 1307330211, 1987219652,  470780820, 2766161472, 3588426471, 1604538578, 1534381854, 1439059739, 424157219,  932997553,  891055288, 3278724476, 3282557696, 1182991381, 2070630603, 2367224474,   52311777, 3530625199, 2369495239,  586341285, 1591670278, 1842466993, 2437252703, 1843447969, 2708260881, 1669471089, 3809723427,  128024980, 1813795929, 1052270510, 1874058909,  979128452, 1360947469, 915070646, 3035841737,  425183251, 1983454076, 2963848783, 2961525344, 1511181226, 1310056056, 2751367103, 1547350932, 2455148493, 2383457456, 3386729792,  677302134,  514326678, 1286767159,  563496536, 3092699587, 2501063473,   22970156, 646625642, 1004200417, 3647178709,  114302591, 2045181997, 1215209071, 2205114490, 3101247597,   75889633, 1271843292, 1403513417, 2778199937, 2969486025,  575569458,  366906582, 3691386763,  364319174, 2384919101, 1560638203,  981739459, 700451857, 3216039128, 1223544102,  413978760, 1266618062, 2209783762, 3510853176,  345948528, 2721530458, 2192983523, 4067818481, 3355371943, 3860263650, 2670673140, 2281485374, 550189277, 2074522979, 1300562653, 2818956800, 1479387647, 133103724, 2608396243,  101687213,  442013050, 1531397212, 1094631148, 2255038909,  118337688,  269485366, 3583374733, 594862150,  856421087, 1086925373, 2929444591, 2909098437, 3526214600, 1751311197,  573829267, 3795949127,  690400797, 3996858299, 3560338659, 2418539948, 2655728883, 3516390755, 2214770735, 2396897297, 1305211856, 2913536967, 1767969548, 2279176821, 2291663176, 2108628360, 3886283108, 1778025979, 56600315, 4193662305,  778208005, 3055443797, 1784819120, 1862713651, 3412851737, 3896140194, 1957605094, 3235225989, 1516466497, 4187418532, 1079897416, 4060721203, 3194762327, 1392845809, 3167919897, 4093327889, 3803155589, 3240110154, 1295906618, 1230905013,  394031731, 2189276111,  661366354, 1303937484, 1876749396, 2935340029, 1082631184, 3032995115, 1678314404, 1011869329,  132157870, 2907805984, 2484248037, 657058975, 1870951224,  326760443, 2695965318,  256011156, 4057114341, 3149328448, 1603149197, 2679477696,  538372367, 2670459761, 3121594598, 1286414204, 2123518682, 3859735793, 386127621,  532185989, 4140896774, 3712207495, 3434843043, 1964423088,  203109222, 3305561808, 2226771410, 1768795300, 2653711278, 2933491826, 1625736647, 2873949878, 2962936477, 1579495765, 3579734975, 4161543880, 3570387636, 3900173009, 3887597689, 2608505794, 1933859937,  369611650, 2178221549, 918634479, 2566791664, 1644947488, 1389576108, 1948693993, 1677127217, 1496558234, 3857466322,  758684705, 1405764530, 2867626573, 3845435866,  166088013, 3226842642, 3870748519, 2818897959, 1472509261, 3937744049,  462360048,  259387751, 1166630491, 2133222715, 4075289500, 4035403382, 1210573541, 879004081, 3912306067, 1603038095, 2966908614, 3332479253, 1346907504,  920807512, 3415360636,  181796576, 1140027430, 2602639402, 2942245264, 1613904483, 3317529716,  111070142, 1373704642, 2158124732, 1677004996,  655101064, 2452470648, 3182772979, 1508729831, 1214808465, 3380686756, 1678323128, 2904952019,  848685388, 2685581524, 2183623751, 3232550419, 3773029931, 2665615268,  521798747, 3021302981, 1897622370, 4294859503,  712387837, 4120782515, 3639178579, 2031295092, 317001486, 1523544172, 3858543080, 4183160479, 3995584885, 2327618424, 2688469773, 3034195415, 4246924278, 1589655191, 86364445,  949946209, 2438452278,  362974838,  882465921, 3330902489, 2885619447,  525665695,  400969906, 4226488882, 2552008782, 1318165494, 1672132974, 2510328263,  247394742, 1688847564, 1860656096,   10070725, 1080479418, 4165903561, 3334283189, 1150282450, 2167902788, 1075719588, 1306022176, 2645142797,  284643316,  721953102,  623502289, 2066267265, 2339561613, 4134019449, 1398112561, 1798220288, 1416273270, 3734360325, 1025617777, 2638525097, 3596944012,  169970561, 795546876, 3864929668, 1072369500, 1048137062, 3054575961, 1293475331, 2746002151, 1121385335,  329080237, 2146602081, 2658190337, 2071458385, 1194290099, 2151006558, 1932756849, 4208943446,  176874928,   92786840, 2975770145, 4030023929, 3219756749,  746997975,  534171136,  637356078, 3922729557, 2643225630,  319137085, 3344095299, 3811152755,  660479332, 3818397353, 3256030536,   16354254, 2833737854, 2778371877, 2800911261, 1400309494,   91287727, 4280655432, 3281688789, 1570952365, 1975645724,  956735666, 2289740412, 1326756857, 3544365852, 1095789696,  844290603, 1285429310,  319965731, 450428184, 3266688194, 1823644528, 1673383649,  364413101, 3479329679,   94375051, 3668465703, 4103212165, 2247928112, 3415770381, 2109931226, 1414447548, 1894487783, 3372065045, 3068089862, 3040941549, 3304296613,  151710169, 1499620857, 1445721397,  704418786,  519589001, 4239033709, 4272981191, 1141721014,  513752214, 1012477842, 1856170239, 1541921948, 2614432088, 4073653482,   61645654, 2514450776,   43125997, 2198139117, 3228301240, 1548276277,  279233779], 624, 0, 0.0)\n",
    "#state2=('MT19937', [2872343671, 2239023895, 1989005992, 2519389960, 1606100909, 3718771305, 1842980521, 2910468440, 3807414888, 1791789786, 796701347, 1373523649, 2316986294, 1648370832,   80477298, 2429270491, 1461550353, 3135038352, 1057777923, 3282306136, 2066801348, 3507729908, 3363055919,  235873889, 3687290411, 1629795779, 1449038583, 2347310395, 3301081224, 2739876235, 390198169, 2024285591,  772626887, 3462221287, 2717173637, 1548630915, 3095782304, 3463095357, 1575543138, 3672058273, 124560235,  248673361, 2663051704, 1518250015, 1307946063, 4108870795, 2132301863, 2596842470, 3717329718, 3142873513, 3826945492, 2480090006, 4161855916,  825588097, 3627521505, 4185330975, 2714407537,  532143429, 3805926185, 1752997498, 1943772472, 1520634692,  304769174, 2787480520, 1475739457, 3042506024, 1682214385,  293467540, 1860510046, 1505627001, 389311173, 1288126802,  915714518,  470975160, 1388767631, 193983247, 1912840774,  774566555, 2861275107, 2567286045, 3555208892, 2117452167, 1163247094, 1934251635, 1776123907, 2054280122, 1662002180, 1551175746, 1154510021, 1370067042, 857649990,  849707466, 1268862530, 1134419405, 2338487878, 1708639166, 1275266410, 3555368935, 3172739332, 3025233296, 112495285, 1483577202, 1032804170, 3263930936, 3465733140, 2528042874,  719213582, 3752763154,  504200721, 1038809666, 2046047289, 3743054485, 2452866448, 2480590673, 2034047929, 1909631940,  973321500, 2444465768, 3137021070,   67128704, 3220329826, 1930231524, 3613011773,  481016976, 1546476627, 1519536415, 2252125896, 1466506080, 2017905669, 2095481875, 3114227650, 1672869881,   78803962, 4224855099, 4046761469, 1486401653,  406519656, 3008273607, 3810100136, 3661869680, 1274863943, 3433192237, 2336228046,  382682823, 1176801592, 1085641218, 2916957023,  119247612, 1202067749, 2576529665, 2751064786, 2683068573, 3682017146, 1304037380, 1681904478, 1345271897,    6508825, 3840877025, 1328677034,  601718995, 1859151099, 4033070288, 3485059421, 1915480481,  975424384, 3440266098, 3600691637, 1524804385, 3802778272,   39272285, 2773916078, 1750671338,  716893672, 2031871706, 1195031601, 4110632070, 3882134969, 1402460760, 3207733890, 1883333755, 4137608199,  348038680, 2220999327, 1800913868,  348953075, 2364936164,  610485089, 1128920976, 1664189106, 2358652414, 2563886991,  343834205,  545281667,   52686237, 3068336899, 4081237891, 4104395190, 1670992989, 2982790392,  488323580, 2785494253,  190176421, 4048793508, 2060347298,  245680814, 1866605039,  346514631, 3423184827, 1504735352, 3286259045, 1031314860, 1942184226, 2383521544, 3483813359,  902055134, 135453513, 2686786406, 2147280730, 2803123838, 4050720510, 553238214, 1875361570,  566524292, 1026041051, 3516769215, 4032180353, 2672550932, 4059192061, 3045006501, 1244605551, 2036326790, 4082944913,  785641338, 4083686186, 2322571560, 1109718107, 3571190294, 1193726502, 2869637704,   17489544, 4249129456,  456865682, 4148921342,  637503672, 3119095100, 4002106526,  502618758, 2187733461, 4270217904, 1289332633, 1614369510, 1154292621,  216096049, 1957551274, 4187998553, 3851893137, 1174560490, 1060156418, 2960365849, 3890590327, 2673761163, 3646023922,  866899706, 2449946864, 3733760935, 1410500635,  935319656, 1172451950, 4280919166, 1662547212, 2110056853, 3413047884, 2478922723, 4163224373, 2245361902, 4201517562, 2601408883, 3396927748,  829088578, 2487723015, 3371613627, 1805067541, 1400171988, 1601857192, 2489530122, 668439939, 1884020407, 2597456880,  203260025,  421749976, 2587425822, 2759751809, 2644214464, 1237016592, 3724162116, 960373592, 2943380332, 3589358719,  522990276, 1097821437, 4176601694, 4112626246, 1175283715, 3124759887,  561645100, 2172510266, 3981622224, 3571048916,  773731522, 2708598450, 4067525585,  184306945, 1428619104,  132051894, 1535637517, 2233696468,  418282044,  331155217, 3794275742, 1198460481, 3823212525, 2275235606, 2291344879, 2193899801, 3027829254, 54382899,  865923616, 2559568275, 3644805939, 2256882089, 1567994619, 4182970977, 1143315281, 3701379937, 1119619279, 816934812, 3303335605, 3337461044, 2946511280, 4292967332, 2976154358, 1217864109,    5067080,  643468278, 1968792208, 678416194,  453333279, 3419363461,  311338391, 1214503335, 2685465455, 1362913456, 3573002820, 2999336700, 1747213598, 1128953835, 3396784264, 2373083026,  221021612, 2911333571, 3218348416, 1737750997,  279975620, 1579961456,  244737626, 3051554159, 3062737950,  201231784,  692399554,  103397960, 505858314, 4203117537, 1564039135, 2046988667, 3820087752, 3580101148, 2867799064, 1560243920, 2225926282, 2077020627, 3991375190, 3664005258, 1051607519, 3302282865, 3514916560, 401629493, 3716444522, 2361213363, 1737906724, 3281826121, 4239300873,  969394051, 3559104718, 2746224447, 3936735189, 3540916985, 1162525714, 1680880092, 1592463450, 4038655080, 1218628598, 3960865899, 2994607193,  626996189, 2415614079, 2573234487, 1824120655, 1560701690, 3219150326, 1260742857, 219924826, 1983641861, 4138402062, 1433495724, 3378480368, 2935018391, 1991818446, 3527378612, 2399267926, 3235975397, 1937894858, 3964934003,  876826023, 2070613882, 2911891147, 2877770656,  435301236,  874313811,  841384658, 1190793495, 625235540, 4127700326, 2742804206, 2085658230, 3767735994, 165740752, 1263627920, 1369476132, 3890210925,  536847811, 490731576, 1806130311, 2182337857, 1621619574, 3750415221, 2937236915, 1840007646, 1756264508, 3537544944, 1275342429, 600822476, 4093014837, 3040058922, 3549713437, 3094487732, 3298361209, 3767875011, 3614070489,  273457233, 1269848997, 1539900681, 1236172784, 1016574714,  785177614, 2889308382, 239925473,  628350545,  322601570,  528235190,  791570180, 819553558, 2761521320, 1494699549, 2971014835, 3598260114, 3447309517, 3801870998, 1315674109, 3452879500,  125323729, 150685509, 1945057444, 1602952519, 1171706958, 3714989465, 2451375305, 2226082128,  167559914, 3638164108, 1846699012, 2847590237,  492443321,  518546710, 1530666363, 1855059200, 554817445, 2098638243, 3048852471, 1623464716,  824667429, 2410210184, 3029332487,  179320634,  972736749, 3067177141, 4245991787, 3126114264,  726190481, 3345048313,  845358113, 1573285908, 1522157609, 1056559185, 1603990601, 4003136040, 2327378734, 3147652809,  609278675, 4150308359,  432741536, 3699214162,  726269077, 3579135054, 1094004351, 1579077086, 3647958466, 2541259356, 1726695449, 1218609475, 2378549859, 1189672992, 1131351963, 2674455801, 2259579303, 1648379464, 1178154438, 1084995162, 1837440578, 2668764331, 2727473305, 3384791022, 3874358880, 2775894523, 1045910750, 2161478179, 1971904187,  306066589, 1715485018, 1076327451, 1941842975, 147249575, 1174333010, 1035303606, 2266202353, 3554713895, 3284538924, 1831080762,  217115355, 3223460337, 3483943785, 2980907826, 3740329512,  990958984, 1942751237, 4028581680, 2670153083, 3460373537,  328215257, 3297560526, 1896870723, 3731249177, 3975637406, 3400466463, 3211314440, 3136634400, 526953092, 1658964932, 1938691023, 2458654626, 3650816692, 3061976247, 2784498231,  992979644, 1078398631, 3800180309, 2568765309,  832386294, 2273144750, 3091269997,  907870210, 657672074,   45109165,  900580771, 2972537174, 1098311713, 3821436755, 2460509233, 1783159618, 2556908948, 1644017023, 429207801, 3182413837, 2442009295,  589954039,  368380036, 2825496450,  775673615, 1131700493, 3483610443, 2623846017, 19786647, 4153878607, 1962373074, 3952548886,  434727237, 1761500584, 3383161218, 3330604458, 1211598432, 1514838979, 35406292,   75391790, 1890474237, 2312647414], 44, 0, 0.0)\n",
    "#state3=('MT19937', [2108211128, 2527430299, 4192017223, 2625402879, 3174555681, 2022678817, 2680360192,  687524803,  789983273, 2559032048, 1061351843, 3907993216, 2274468456, 1646363939, 2553621988, 1967080462, 3390661242,  489435987, 3377982090, 3819426765, 2403597218, 2400012891, 1091163126, 3797853834, 4185460117, 4031381599, 1162567273, 1491538376, 3389268462, 3579154880, 2813542789, 2779274534,  214652136,  974169338, 3783704622, 2615002183, 3808356746, 1186467645,  654087330, 2572617128, 292098669, 1623059241, 3516970443, 3046522805, 2182150295, 2408924925,  727368013, 3280097587,  806363758, 2238248469, 1981245307,  964950666, 3982297489, 3290915655, 1890884893, 181690625, 2599059891, 1179164902, 2553705857, 1059991057, 4199608049, 2001070458, 3294358102,  306333077,  723826508, 1611778045,  399012637, 3951775085, 2321903680, 1777277873, 98125490,  511201609, 1814560373, 2576718381, 4158404049, 1433640578,  920052697, 3251859353, 4133599423,  782901469, 2993506619, 1071111702, 1925203904, 1660956907, 1686103911, 494434633, 3596361235, 1574842614,  945643900, 2191525063, 105156582, 3652152993,  968037704, 3137791607,  865559392, 3945040168, 1141623331, 1430622273,  110623811, 1718518439, 1035216736, 1087457138, 3868303794, 2909754173, 1972200715, 3681844293, 3608669568, 2423844206, 1504309087,  657859171, 3488251429, 3014939202, 4132001620, 3850607644, 1139049802, 1082107566, 1275988679, 1086467220,  804164628, 2281639504, 3615059499, 4160469798, 2736290919, 3363335257, 3417667264, 4061779585, 3395559934, 2146057535, 1974337447, 2999032146, 2312036576, 4166427877, 2510950347, 2588072156, 3391743315, 396258423, 2099316707, 3828309440, 4065148968, 1400340539, 3311742139, 1772401249,  183195249, 4131114934, 2571570611, 2639323152,  689610514, 3347120954, 3318589318, 3533537723, 178825461, 3149458077, 1738672616, 1883281519,  486825440, 3042045025, 1192697823, 2516415278,  999934003, 4218100339, 3961921282, 3478708235,  418698777, 2027724052, 3457585540, 1952805688,  725580342, 2067074748,  986043452, 3004018845, 1581084803, 1783939526, 4120508539,  379304578, 2476788011, 1904556551, 2837766117, 1457212402,  595024117,  601163942, 4055770621, 2628986244, 3461380316,  924710345, 1046943985, 1232467372, 2548339570, 3442033102, 3257747650,  618783007, 1810556078, 2757258952,   92263296,  977718763, 4045339016, 4050996239, 4082130739, 1024033951,  403275062, 2451238233, 2931697124,  190791769, 1290298498, 1842249583, 2886765473, 3808813389, 3633569851, 2723940308,  911780196, 2570721751, 795298783,  988269220, 1377512993, 3997777568, 3356629907, 4217693401, 2515842228,  553972077, 2192487042, 3995250221, 3797090983, 2745687960, 2764238015, 1939505332, 3392432398, 323737314, 3608612438, 2265358502, 2774960106, 1629014827, 61753951, 2469040051,  546757442,  585785223, 1823750681, 2716207879, 3261807884, 3849154842, 3067029964, 1658529734, 1803333941, 4077881701, 3559487879, 3002827510, 3699472086, 1314756722,  825508683, 1901493230, 4232452041,  680094928, 3067281358, 3322871163, 4088062714,  511036671, 1209002524, 4106518276, 1592519891, 3288681016,  181936271,  744489648, 557326180, 2784656729, 2612192269,  867343332, 2431853971, 2096331853, 3571112273, 1863569619,  746559439, 3433603465, 1406906813, 1813755522,  668223369, 3111633112, 2679142124, 2925976690,  484084533, 2801128897, 2075385091, 4049679332, 685192293,  924238170, 1316150886, 3129650176, 1710284877, 2149708551, 1365528054, 2654747247, 2548313685, 3879181058, 2906579212, 3499810762, 3305248716,  400089269, 3694511090, 2956212538, 3319643443, 1301117711, 4039294822,  391713788, 2427432361,  319636754, 3091952140, 1317429426,  905140594, 2703007861, 3176050470,   35626495, 3522457115, 2659274794, 4051746132,  659003405, 3945646157, 3350807117, 3066037377, 2048290665, 2499527840,  735403925,   97765384, 4158619651, 1084260382, 1857774622, 2597984738, 1471162735, 3240155620, 1769378467, 1172117815, 1681267160, 2204169107,  447996314, 3145107969, 2041966415,  557441701, 2451346704,  440982736, 2847720999,  256391291,  176593614, 2455716607,   48703866, 2378444445,  236555167, 3564783692,  785715672,  113081979, 2080828578, 1987503170, 2577402027,  977034130, 2453773068, 4289885312,  165612196, 3816125300, 1209091645, 3602196786, 2740753984,   62624679, 3773673455, 2726329129, 3504645825, 2216598383, 2704259081, 3744605582, 2158044142, 2701544741, 1607860008, 2308682230, 3956800591, 1547041200, 2820919929, 1339793351, 2265304255, 3679402875,  527186701, 2314833627, 3011445892, 3460879416, 1527791943,  881945393,  491681403, 1140952913, 1435974984,  573044504,  868367355, 4023300740, 316652323, 1346655812, 1369258211, 3340283052, 4023420326, 3245754279,  921853931, 4089688143, 1528643438, 2630389314, 3062538964,  460504303, 1695817083, 1085454948, 3349793710, 3828186247, 2520216885, 4208136182,  339431303, 1443438967, 2018226642, 1270306008,  771039641,  332479130, 4110799016, 455679854, 3323345398, 3431819626,   50095539,  687997097, 890761220, 1209528505, 3748522798, 2565241538, 2697849588, 1072583840, 3797698459, 2955238572,  784497924, 3735794565, 27428451, 2713189150, 4019579977,  417738450, 3084334794, 1194996394, 3652279977, 2507080558,  101481278, 2375028714, 2955067690, 1711609550, 4272039316, 3832490641, 3354958512, 3458372628, 3103983613, 1223856194, 1680998300, 2247996131, 1970828101, 3911091681, 1816830356, 3761314569,  238212915, 2513079548, 4047215625, 3516804536, 3183502235,  384782634, 174531601, 1673486073, 1576853331,  182031195,  395023627, 3730780759,  172328330, 2490846282, 2327297234, 1818232020, 4269248683, 4248977035, 3569924383,  909240122, 2605273357, 1998608194, 1510641474, 1641790130, 1172833791, 2522032074, 2171357350, 1717167697, 1190619983,  107030942, 3096952759, 3108852545, 1215931034, 4196006042, 3068024557,  261372072, 467529116, 2061592456,   63332911,  782276471,  826944233, 1821021470, 3563239388, 2533712700, 1905686700, 1458348968, 492358119, 2536567081,  544777088, 2340468877, 1793868296, 2818596855, 1526639114, 1308284632, 4016518676, 3639315966, 1400769388,   20271569, 2559166868, 3190616761, 4068831899, 641939100,  728692401, 3825929579,  421841888, 1707495467, 3871324694, 3324929206, 1643446389,  389579305, 3697673575, 1621372114,  429142037, 3212140329,   69212767, 1207744546, 1778464804,  595001801, 2740933585, 2257421564, 2239849438, 1940338110, 3710025581, 1988303760, 4064464883,  136684297, 337416451, 2398508180, 3383131018, 3288574218, 3747438177, 2238360023,  474174658,  750849410,  554690679, 2832180947, 3016105702, 1646209648, 3091808314, 1236224803,  434619131, 177209167,  624010569, 4025481646, 2799578942, 2609193957, 1658816594, 2726871629,  945244465, 1091392649, 3023235044, 3861410548, 3709494645,  656796139, 3430081417, 3344912019, 989366657, 1930649093,  783375650, 1454380084, 3129971147, 1033301807, 1247869205,  352989108,  791305219, 1078403938, 2458717678, 1545136945, 2940146123, 1430547443, 2701507027, 3885238974,  455411733,  536077325, 4007875855,  772681747, 3230830266,  187871694, 2607126894, 2876437473, 2032673873, 1741428585,  205756050, 3675082053,  471009104, 3451833413, 4163447888, 2673521296, 3284603168, 1259649339, 2487790146, 468866744, 1956220276, 2070821947, 1850974543, 3335171295, 3676813206, 2687343829,  241570264,  848224894,  503016090, 644703484, 3201812132, 2225511387, 1096607806, 2265814463, 2533212665,  271020420, 3381550487,  356587068, 3115740140, 3660253096, 2416512950,  305366375, 1705435955], 91, 0, 0.0)\n",
    "\n",
    "total=0\n",
    "\n",
    "#uncomment to reproduce results\n",
    "#np.random.set_state(state1)\n",
    "\n",
    "# Initializing the multilayer perceptron\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(15,5),solver='sgd',learning_rate_init= 0.008, max_iter=1000,tol=0.000000001,verbose=True)\n",
    "#print(np.random.get_state())\n",
    "start = timeit.default_timer()\n",
    "mlp.fit(X_train, y_train)\n",
    "stop = timeit.default_timer()\n",
    "total=total+(stop-start)\n",
    "y_pred1 = mlp.predict_proba(X_test)\n",
    "pred1=np.argmax(y_pred1,axis=1)\n",
    "\n",
    "#uncomment to reproduce results\n",
    "#np.random.set_state(state2)\n",
    "\n",
    "# Initializing the multilayer perceptron\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(15,5),solver='sgd',learning_rate_init= 0.008, max_iter=1000,tol=0.000000001,verbose=True)\n",
    "#print(np.random.get_state())\n",
    "start = timeit.default_timer()\n",
    "mlp.fit(X_train, y_train)\n",
    "stop = timeit.default_timer()\n",
    "total=total+(stop-start)\n",
    "y_pred2 = mlp.predict_proba(X_test)\n",
    "pred2=np.argmax(y_pred2,axis=1)\n",
    "\n",
    "#uncomment to reproduce results\n",
    "#np.random.set_state(state3)\n",
    "\n",
    "# Initializing the multilayer perceptron\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(15,5),solver='sgd',learning_rate_init= 0.008, max_iter=1000,tol=0.000000001,verbose=True)\n",
    "#print(np.random.get_state())\n",
    "start = timeit.default_timer()\n",
    "mlp.fit(X_train, y_train)\n",
    "stop = timeit.default_timer()\n",
    "total=total+(stop-start)\n",
    "y_pred3 = mlp.predict_proba(X_test)\n",
    "pred3=np.argmax(y_pred3,axis=1)\n",
    "\n",
    "y_pred = np.zeros((len(pred1)),dtype=int)\n",
    "\n",
    "\n",
    "for i in range(len(pred1)):\n",
    "    if pred1[i]==pred2[i] or pred1[i]==pred3[i] or pred2[i]==pred3[i] :\n",
    "        if pred1[i]==pred2[i] or pred1[i]==pred3[i]:\n",
    "            y_pred[i]=pred1[i]\n",
    "        else:\n",
    "            y_pred[i]=pred2[i]\n",
    "    else:\n",
    "        \n",
    "        max1=np.amax(y_pred1[i])\n",
    "        max2=np.amax(y_pred2[i])\n",
    "        max3=-np.amax(y_pred3[i])\n",
    "           \n",
    "        if max1>max2 and max1>max3:\n",
    "            y_pred[i]=np.argmax(y_pred1[i])\n",
    "        elif max2>max1 and max2>max3:\n",
    "            y_pred[i]=np.argmax(y_pred2[i])\n",
    "        else:\n",
    "            y_pred[i]=np.argmax(y_pred3[i])\n",
    "\n",
    "print('Time: ', total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "with open('y_pred.csv', 'w', newline='') as csvfile:\n",
    "    writer = csv.writer(csvfile, delimiter=',')\n",
    "    writer.writerow(['Id', 'Label'])\n",
    "    for i in range(y_pred.shape[0]):\n",
    "        writer.writerow([i, y_pred[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.549438202247191"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "f1_score(y_test, y_pred, average='micro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Υποβάλλουμε το αρχείο `y_pred.csv` στην πλατφόρμα και μας δίνει macro F1-score ίσο με 0.33. Στα πλαίσια της παρούσας εργασίας καλείστε να τροποποιήσετε τον παραπάνω κώδικα ώστε να προβλέψετε τις κατηγορίες αριθμού επιβατών των πτήσεων του test set. Μπορείτε να εφαρμόσετε κάποια μέθοδο επιλογής χαρακτηριστικών στα δεδομένα ώστε να κρατήσετε\n",
    "μόνο ένα υποσύνολο από τα χαρακτηριστικά. Μπορείτε επίσης να δημιουργήσετε νέα χαρακτηριστικά τα οποία\n",
    "ίσως βοηθήσουν στην κατηγοριοποίηση. Μπορείτε επιπλέον να πειραματιστείτε με κάποια μέθοδο μείωσης\n",
    "διάστασης και να διερευνήσετε αν η εφαρμογή της βελτιώνει το αποτέλεσμα της κατηγοριοποίησης. Επίσης,\n",
    "μπορείτε να χρησιμοποιήσετε θορυβώδη ή ανούσια χαρακτηριστικά για να παράγετε νέα χαρακτηριστικά που\n",
    "παρέχουν μεγαλύτερα ποσοστά πληροφορίας. Μπορείτε να χρησιμοποιήσετε διαφορετικούς ταξινομητές ή να συνδυάσετε τα αποτελέσματα περισσότερων από έναν ταξινομητές. \n",
    "\n",
    "### Παράδοση Εργασίας\n",
    "\n",
    "Η εργασία είναι είτε ατομική ή μπορεί να γίνει σε ομάδες το πολύ 3 ατόμων. Η τελική αξιολόγηση θα βασίζεται τόσο στο micro F1-score που θα επιτύχετε, όσο και στη συνολική προσέγγισή σας στο πρόβλημα. Στα πλαίσια της εργασίας, θα πρέπει να υποβληθούν τα εξής:\n",
    "<ul>\n",
    "    <li>Μια αναφορά 2 σελίδων, στην οποία θα πρέπει να περιγράψετε την προσέγγιση και τις μεθόδους που χρησιμοποιήσατε. Δεδομένου ότι πρόκειται για ένα πραγματικό πρόβλημα ταξινόμησης, μας ενδιαφέρει να γνωρίζουμε πώς αντιμετωπίσατε κάθε στάδιο του προβλήματος, π.χ. τι είδους αναπαράσταση δεδομένων χρησιμοποιήσατε, τι χαρακτηριστικά χρησιμοποιήσατε, εάν εφαρμόσατε τεχνικές μείωσης διάστασης, ποιούς αλγορίθμους ταξινόμησης δοκιμάσατε και γιατί, την απόδοση των μεθόδων σας (macro F1-score και χρόνο εκπαίδευσης), τυχόν προσεγγίσεις που τελικά δεν λειτούργησαν, αλλά\n",
    "είναι ενδιαφέρον να παρουσιάσετε, και γενικά, ότι νομίζετε ότι είναι ενδιαφέρον να αναφερθεί.</li>\n",
    "    <li>Ενα φάκελο με τον κώδικα της εφαρμογής σας.</li>\n",
    "    <li>Εναλλακτικά μπορείτε να συνδυάσετε τα δυο παραπάνω σε ένα αρχείο Ipython Notebook.</li>\n",
    "    <li>Δημιουργήστε ένα αρχείο .zip που περιέχει τον κώδικα και την αναφορά και υποβάλετέ τον στην πλατφόρμα e-class.</li>\n",
    "    <li>**Λήξη προθεσμίας**: 6 Ιανουαρίου 2019.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
